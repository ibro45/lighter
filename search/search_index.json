{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Title","text":""},{"location":"#_1","title":"Title","text":"<pre><code>pip install project-lighter\n</code></pre> <p>Lighter is a PyTorch/Pytorch Lightning framework for streamlining deep learning experiments though configuration files.</p> <p>Say goodbye to messy scripts and notebooks. Lighter is here to help you organize your experiments and make them reproducible.</p>"},{"location":"#features","title":"Features","text":"<ol> <li>Structured - the pre-defined structure ensures that your experiments are standardized, organized, and reproducible.</li> <li>Boilerplate-free - forget about writing training logic over and over. <code>System</code> handles it for you and is compatible with any task, from classification to self-supervised learning.</li> <li>Readable - wondering what learning rate or architecture was used? Just glance at the config.</li> <li>Easily modifiable - override parameters from CLI or using another config. Leave <code>argparse</code> and hardcoding in the past.</li> </ol>"},{"location":"#comparison","title":"Comparison","text":"PyTorch Lightning Terminal<pre><code>python cifar10.py\n</code></pre> cifar10.py<pre><code>from pytorch_lightning import Trainer, LightningModule\nfrom torch.nn import CrossEntropyLoss\nfrom torch.optim import Adam\nfrom torch.utils.data import DataLoader\nfrom torchvision.models import resnet18\nfrom torchvision.datasets import CIFAR10\nfrom torchvision.transforms import (ToTensor\n                                    Normalize,\n                                    Compose)\n\nclass Model(LightningModule):\n    def __init__(self):\n        super().__init__()\n        self.model = resnet18(num_classes=10)\n        self.criterion = CrossEntropyLoss()\n\n    def forward(self, x):\n        return self.model(x)\n\n    def training_step(self, batch, batch_idx):\n        x, y = batch\n        y_hat = self(x)\n        loss = self.criterion(y_hat, y)\n        return loss\n\n    def configure_optimizers(self):\n        return Adam(self.model.parameters(), lr=0.001)\n\n\ntransform = Compose([\n    ToTensor(),\n    Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])\n])\n\ntrain_dataset = CIFAR10(\n    root=\".datasets\",\n    train=True,\n    download=True,\n    transform=transform\n)\n\ntrain_loader = DataLoader(\n    train_dataset,\n    batch_size=512,\n    shuffle=True\n)\n\nmodel = Model()\ntrainer = Trainer(max_epochs=100)\ntrainer.fit(model, train_loader)\n</code></pre> Lighter Terminal<pre><code>lighter fit --config cifar10.yaml\n</code></pre> cifar10.yaml<pre><code>trainer:\n    _target_: pytorch_lightning.Trainer\n    max_epochs: 100\n\nsystem:\n    _target_: lighter.System\n    batch_size: 512\n\n    model: torchvision.models.resnet18\n    num_classes: 10\n\n    criterion:\n    _target_: torch.nn.CrossEntropyLoss\n\n    optimizer:\n    _target_: torch.optim.Adam\n    params: \"$@system#model.parameters()\"\n    lr: 0.001\n\n    datasets:\n        train:\n        _target_: torchvision.datasets.CIFAR10\n        download: True\n        root: .datasets\n        train: True\n        transform:\n            _target_: torchvision.transforms.Compose\n            transforms:\n            - _target_: torchvision.transforms.ToTensor\n            - _target_: torchvision.transforms.Normalize\n            mean: [0.5, 0.5, 0.5]\n            std: [0.5, 0.5, 0.5]\n</code></pre>"},{"location":"#cite","title":"Cite","text":"<p>If you find Lighter useful in your research or project, please consider citing it:</p> <pre><code>@software{lighter,\n  author       = {Ibrahim Hadzic and\n                  Suraj Pai and\n                  Keno Bressem and\n                  Hugo Aerts},\n  title        = {Lighter},\n  publisher    = {Zenodo},\n  doi          = {10.5281/zenodo.8007711},\n  url          = {https://doi.org/10.5281/zenodo.8007711}\n}\n</code></pre>"},{"location":"advanced/callbacks/","title":"Callbacks","text":"<p>Callbacks in Lighter allow you to customize and extend the training process. You can define custom actions to be executed at various stages of the training loop.</p>"},{"location":"advanced/callbacks/#freezer-callback","title":"Freezer Callback","text":"<p>The <code>Freezer</code> callback allows you to freeze certain layers of the model during training. This can be useful for transfer learning or fine-tuning.</p>"},{"location":"advanced/callbacks/#writer-callbacks","title":"Writer Callbacks","text":"<p>Lighter provides writer callbacks to save predictions in different formats. The <code>FileWriter</code> and <code>TableWriter</code> are examples of such callbacks.</p> <ul> <li>FileWriter: Writes predictions to files, supporting formats like images, videos, and ITK images.</li> <li>TableWriter: Saves predictions in a table format, such as CSV.</li> </ul> <p>For more details on how to implement and use callbacks, refer to the PyTorch Lightning Callback documentation.</p>"},{"location":"advanced/inferer/","title":"Inferer","text":"<p>The inferer in Lighter is used for making predictions on data. It is typically used in validation, testing, and prediction workflows.</p>"},{"location":"advanced/inferer/#using-inferers","title":"Using Inferers","text":"<p>Inferers must be classes with a <code>__call__</code> method that accepts two arguments: the input to infer over and the model itself. They are used to handle complex inference scenarios, such as patch-based or sliding window inference.</p>"},{"location":"advanced/inferer/#monai-inferers","title":"MONAI Inferers","text":"<p>Lighter integrates with MONAI inferers, which cover most common inference scenarios. You can use MONAI's sliding window or patch-based inferers directly in your Lighter configuration.</p> <p>For more information on MONAI inferers, visit the MONAI documentation.</p>"},{"location":"advanced/postprocessing/","title":"Postprocessing","text":"<p>Postprocessing in Lighter allows you to apply custom transformations to data at various stages of the workflow. This can include modifying inputs, targets, predictions, or entire batches.</p>"},{"location":"advanced/postprocessing/#defining-postprocessing-functions","title":"Defining Postprocessing Functions","text":"<p>Postprocessing functions can be defined in the configuration file under the <code>postprocessing</code> key. They can be applied to: - Batch: Modify the entire batch before it is passed to the model. - Criterion: Modify inputs, targets, or predictions before loss calculation. - Metrics: Modify inputs, targets, or predictions before metric calculation. - Logging: Modify inputs, targets, or predictions before logging.</p>"},{"location":"advanced/postprocessing/#example","title":"Example","text":"<pre><code>postprocessing:\n  batch:\n    train: '$lambda x: {\"input\": x[0], \"target\": x[1]}'\n  criterion:\n    input: '$lambda x: x / 255.0'\n  metrics:\n    pred: '$lambda x: x.argmax(dim=1)'\n  logging:\n    target: '$lambda x: x.cpu().numpy()'\n</code></pre> <p>For more information on how to use postprocessing in Lighter, refer to the Lighter documentation.</p>"},{"location":"basics/config/","title":"Configuration System","text":"<p>Lighter is a configuration-centric framework that uses YAML files to set up machine learning workflows. These configurations cover everything from model architecture selection, loss functions, and optimizers to dataset preparation and the execution of training, evaluation, and inference processes.</p> <p>Our configuration system is inspired by the MONAI bundle parser, offering a standardized structure. Each configuration requires several mandatory components to be defined.</p> <p>The configuration is divided into two main components: - Trainer: Handles the training process, including epochs, devices, etc. - System: Encapsulates the model, optimizer, datasets, and other components.</p> <p>Let's explore a simple example configuration to understand Lighter's configuration system better. You can expand each section for more details on specific concepts.</p> cifar10.yaml<pre><code>trainer:\n  _target_ (1): pytorch_lightning.Trainer \n  max_epochs (2): 100\n\nsystem:\n  _target_: lighter.System\n  batch_size: 512\n\n  model:\n    _target_: torchvision.models.resnet18\n    num_classes: 10\n\n  criterion:\n    _target_: torch.nn.CrossEntropyLoss\n\n  optimizer:\n    _target_: torch.optim.Adam\n    params: \"$@system#model.parameters()\" (3)\n    lr: 0.001\n\n  datasets:\n    train:\n      _target_: torchvision.datasets.CIFAR10\n      download: True\n      root: .datasets\n      train: True\n      transform:\n        _target_: torchvision.transforms.Compose\n        transforms: (4)\n          - _target_: torchvision.transforms.ToTensor\n          - _target_: torchvision.transforms.Normalize\n            mean: [0.5, 0.5, 0.5]\n            std: [0.5, 0.5, 0.5]\n</code></pre> <ol> <li><code>_target_</code> is a special reserved keyword that initializes a python object from the provided text. In this case, a <code>Trainer</code> object from the <code>pytorch_lightning</code> library is initialized</li> <li><code>max_epochs</code> is an argument of the <code>Trainer</code> class which is passed through this format. Any argument for the class can be passed similarly.</li> <li><code>$@</code> is a combination of <code>$</code> which evaluates a python expression and <code>@</code> which references a python object. In this case we first reference the model with <code>@model</code> which is the <code>torchvision.models.resnet18</code> defined earlier and then access its parameters using <code>$@model.parameters()</code></li> <li> <p>YAML allows passing a list in the format below where each <code>_target_</code> specifies a transform that is added to the list of transforms in <code>Compose</code>. The <code>torchvision.datasets.CIFAR10</code> accepts these with a <code>transform</code> argument and applies them to each item.</p> </li> <li> <p>Datasets are defined for different modes: train, val, test, and predict. Each dataset can have its own transforms and configurations.</p> </li> </ol>"},{"location":"basics/config/#configuration-concepts","title":"Configuration Concepts","text":"<p>As seen in the Quickstart, Lighter has two main components:</p>"},{"location":"basics/config/#trainer-setup","title":"Trainer Setup","text":"<pre><code>    trainer:\n        _target_: pytorch_lightning.Trainer # (1)!\n        max_epochs: 100\n</code></pre> <p>The trainer object (<code>pytorch_lightning.Trainer</code>) is initialized using the <code>_target_</code> key. For more information on <code>_target_</code> and other special keys, see Special Syntax and Keywords.</p> <p>The <code>max_epochs</code> parameter is passed to the <code>pytorch_lightning.Trainer</code> object during instantiation. You can provide any argument accepted by the class in this manner.</p>"},{"location":"basics/config/#system-configuration","title":"System Configuration","text":"<p>While Lighter utilizes the Trainer from PyTorch Lightning, System is a unique component that incorporates concepts from PL, such as LightningModule, to encapsulate all essential elements of a deep learning system in a straightforward manner.</p> <p>Concepts encapsulated by System include,</p>"},{"location":"basics/config/#model-definition","title":"Model definition","text":"<p>The <code>torchvision</code> library is included by default in Lighter, allowing you to select various torchvision models. Additionally, Lighter includes <code>monai</code>, enabling you to easily switch to a ResNet model by adjusting your configuration as follows:</p> Torchvision ResNet18MONAI ResNet50MONAI 3DResNet50 <pre><code>System:\n  ...\n  model:\n    _target_: torchvision.models.resnet18\n    num_classes: 10\n  ...\n</code></pre> <pre><code>System:\n  ...\n  model:\n    _target_: monai.networks.nets.resnet50\n    num_classes: 10\n    spatial_dims: 2\n  ...\n</code></pre> <pre><code>System:\n  ...\n  model:\n    _target_: monai.networks.nets.resnet50\n    num_classes: 10\n    spatial_dims: 3 \n  ...\n</code></pre> <p></p>"},{"location":"basics/config/#criterionloss","title":"Criterion/Loss","text":"<p>Just as you can override models, Lighter allows you to switch between various loss functions from libraries like <code>torch</code> and <code>monai</code>. This flexibility lets you experiment with different optimization strategies without altering your code. Here are examples of how to modify the criterion section in your configuration to use different loss functions:</p> CrossEntropyLossMONAI's Dice Loss <pre><code>System:\n  ...\n  criterion:\n    _target_: torch.nn.CrossEntropyLoss\n  ...\n</code></pre> <pre><code>System:\n  ...\n  criterion:\n    _target_: monai.losses.DiceLoss\n  ...\n</code></pre> <p></p>"},{"location":"basics/config/#optimizer","title":"Optimizer","text":"<p>Similarly, you can experiment with different optimizer parameters. Model parameters are passed directly to the optimizer via the <code>params</code> argument. <pre><code>System:\n  ...\n  optimizer:\n    _target_: torch.optim.Adam\n    params: \"$@system#model.parameters()\"\n    lr: 0.001\n  ...\n</code></pre></p> <p>You can also define a scheduler for the optimizer as shown below: <pre><code>System:\n  ...\n  optimizer:\n    _target_: torch.optim.Adam\n    params: \"$@system#model.parameters()\"\n    lr: 0.001\n\n  scheduler:\n    _target_: torch.optim.lr_scheduler.CosineAnnealingLR\n    optimizer: \"@system#optimizer\"\n    eta_min: 1.0e-06\n    T_max: \"%trainer#max_epochs\"\n\n  ...\n</code></pre> In this example, the optimizer is passed to the scheduler using the <code>optimizer</code> argument. The <code>%trainer#max_epochs</code> syntax retrieves the <code>max_epochs</code> value from the Trainer class.</p> <p></p>"},{"location":"basics/config/#datasets","title":"Datasets","text":"<p>Datasets are often the most frequently modified part of the configuration, as workflows typically involve training or inferring on custom datasets. The <code>datasets</code> key includes <code>train</code>, <code>val</code>, <code>test</code>, and <code>predict</code> sub-keys, which generate dataloaders for each workflow supported by PyTorch Lightning. Detailed information is available here.</p> <pre><code>System:\n  ...\n  datasets:\n    train:\n      _target_: torchvision.datasets.CIFAR10 (1)\n      download: True\n      root: .datasets\n      train: True\n      transform: (2)\n        _target_: torchvision.transforms.Compose\n        transforms:\n          - _target_: torchvision.transforms.ToTensor\n          - _target_: torchvision.transforms.Normalize\n            mean: [0.5, 0.5, 0.5]\n            std: [0.5, 0.5, 0.5]\n  ...\n</code></pre> <ol> <li>Define your own dataset class here or use existing dataset classes. Learn more about this here.</li> <li>Transforms can be applied to each dataset element by initializing a <code>Compose</code> object and providing a list of transforms. This approach is often the best way to adapt constraints for your data.</li> </ol>"},{"location":"basics/config/#special-syntax-and-keywords","title":"Special Syntax and Keywords","text":"<ul> <li><code>_target_</code>: Specifies the Python class to instantiate. If a function is provided, a partial function is created. Any configuration key with <code>_target_</code> maps to a Python object.</li> <li>@: References another configuration value. This syntax allows access to keys mapped to Python objects. For example, the learning rate of an optimizer instantiated as <code>torch.optim.Adam</code> can be accessed using <code>@model#lr</code>, where <code>lr</code> is an attribute of the <code>torch.optim.Adam</code> class.</li> <li>$: Evaluates Python expressions.</li> <li>%: Acts as a macro for textual replacement in the configuration.</li> </ul>"},{"location":"basics/projects/","title":"Using Lighter in your own projects","text":"<p>Lighter offers a flexible framework for integrating deep learning workflows into your projects. Whether you're starting with a pre-defined configuration or building a custom setup, Lighter adapts to your needs. Here\u2019s how you can leverage Lighter:</p> <ul> <li> Train on your own dataset</li> <li> Train on your data + Add a custom model architecture</li> <li> Train on your data + Add a custom model architecture + Add a complex loss function</li> <li> Customization per your imagination! </li> </ul> <p>Let's start by looking at each of these one by one. At the end of this, you will hopefully have a better idea of how best you can leverage Lighter.</p>"},{"location":"basics/projects/#training-on-your-own-dataset","title":"Training on your own dataset","text":"<p>When reproducing a study or adapting a model to new data, you often start with a pre-defined configuration. For instance, consider the <code>cifar10.yaml</code> example from our Quickstart. Suppose you have a dataset of Chest X-rays and wish to replicate the training process used for CIFAR10. With Lighter, you only need to modify specific sections of the configuration.</p> cifar10.yaml<pre><code>system:\n  _target_: lighter.System\n  batch_size: 512\n\n  model:\n    _target_: torchvision.models.resnet18\n    num_classes: 10\n\n  criterion:\n    _target_: torch.nn.CrossEntropyLoss\n\n  optimizer:\n    _target_: torch.optim.Adam\n    params: \"$@system#model.parameters()\"\n    lr: 0.001\n\n  datasets:\n    train:\n      _target_: torchvision.datasets.CIFAR10\n      download: True\n      root: .datasets\n      train: True\n      transform:\n        _target_: torchvision.transforms.Compose\n        transforms:\n          - _target_: torchvision.transforms.ToTensor\n          - _target_: torchvision.transforms.Normalize\n            mean: [0.5, 0.5, 0.5]\n            std: [0.5, 0.5, 0.5]\n</code></pre> <p>To integrate your dataset, create a PyTorch dataset class that outputs a dictionary with <code>input</code>, <code>target</code>, and optionally <code>id</code> keys. This ensures compatibility with Lighter's configuration system.</p> /home/user/project/my_xray_dataset.py<pre><code>class MyXRayDataset(Dataset):\n    \"\"\"\n    Args:\n    - root_dir (string): Directory with all the images.\n    - annotations_file (string): Path to the annotations file.\n    - transform (callable, optional): Optional transform to be applied on a sample.\n    \"\"\"\n\n    def __init__(self, root_dir, annotations_file, transform=None):\n        \"\"\"\n        Initialize the dataset.\n        \"\"\"\n        self.root_dir = root_dir\n        self.annotations = pd.read_csv(annotations_file)\n        self.transform = transform\n\n    def __len__(self):\n        \"\"\"\n        Return the number of samples in the dataset.\n        \"\"\"\n        return len(self.annotations)\n\n    def __getitem__(self, idx):\n        \"\"\"\n        Return a sample from the dataset.\n        \"\"\"\n        if torch.is_tensor(idx):\n            idx = idx.tolist()\n\n        img_name = os.path.join(self.root_dir, self.annotations.iloc[idx, 0])\n        image = Image.open(img_name)\n        target = self.annotations.iloc[idx, 1]\n        sample = {'input': image, 'target': target}\n\n        if self.transform:\n            sample['input'] = self.transform(sample['input'])\n\n        return sample\n</code></pre> <p>Note: Lighter requires the dataset to return a dictionary with <code>input</code>, <code>target</code>, and optionally <code>id</code> keys. This format allows for complex input/target structures, such as multiple images or labels.</p> <p>Once your dataset is ready, integrate it into the Lighter configuration. The <code>project</code> key in the config specifies the path to your Python code, allowing Lighter to locate and utilize your dataset. Simply reference your dataset class, and Lighter will handle the rest.</p> <p>In the above example, the path of the dataset is <code>/home/user/project/my_xray_dataset.py</code>. Copy the config shown above, make the following changes and run on the terminal</p> xray.yamlTerminal <pre><code>project: /home/user/project (1)\nsystem:\n_target_: lighter.System\nbatch_size: 512\n\nmodel:\n    _target_: torchvision.models.resnet18\n    num_classes: 10\n\ncriterion:\n    _target_: torch.nn.CrossEntropyLoss\n\noptimizer:\n    _target_: torch.optim.Adam\n    params: \"$@system#model.parameters()\"\n    lr: 0.001\n\ndatasets:\n    train:\n    _target_: project.my_xray_dataset.MyXRayDataset\n    root_dir: .\n    annotations_file: label.csv\n    transform:\n        _target_: torchvision.transforms.Compose\n        transforms:\n        - _target_: torchvision.transforms.ToTensor\n        - _target_: torchvision.transforms.Normalize\n            mean: [0.5, 0.5, 0.5]\n            std: [0.5, 0.5, 0.5]\n</code></pre> <pre><code>lighter fit --config xray.yaml\n</code></pre> <ol> <li>Make sure to put an <code>__init__.py</code> file in this directory. Remember this is needed for an importable python module</li> </ol>"},{"location":"basics/quickstart/","title":"Quickstart","text":"<p>Get started with Lighter in just a few minutes! This guide will walk you through the installation and setup process, enabling you to quickly configure and run your experiments.</p>"},{"location":"basics/quickstart/#installation","title":"Installation","text":"<pre><code>pip install project-lighter\n</code></pre> <p>For bleeding edge version, run <pre><code>pip install project-lighter --pre\n</code></pre></p>"},{"location":"basics/quickstart/#building-a-config","title":"Building a config","text":"<p>At the heart of the Lighter ecosystem is a YAML configuration file. This file acts as the central hub for managing your experiments, allowing you to define, adjust, and control every aspect without delving into the underlying code.</p> <p>A Lighter config contains two main components:</p> <ul> <li>Trainer: Manages the training loop and related settings.</li> <li>System: Defines the model, datasets, optimizer, and other components.</li> </ul>"},{"location":"basics/quickstart/#trainer","title":"Trainer","text":"<p>The Trainer section encapsulates all the details necessary for running training, evaluation, or inference processes. It is a vital component of training automation in PyTorch Lightning. For more details, refer to the PyTorch Lightning Trainer documentation.</p> <p>In this section, you can configure various parameters such as the number of epochs, GPUs, nodes, and more. Any parameter available in the <code>pytorch_lightning.Trainer</code> class can be specified here. For a comprehensive list, see the API documentation.</p> <p>Defining this in our config looks something like this <pre><code>trainer:\n  _target_: pytorch_lightning.Trainer\n  max_epochs: 100\n</code></pre></p> <p>For more information see here</p>"},{"location":"basics/quickstart/#system","title":"System","text":"<p>The System component encompasses all elements of a deep learning setup, including the model, optimizer, criterion, datasets, and metrics. This is where the core logic of building deep learning models is defined. System is highly adaptable, supporting tasks like classification, segmentation, object detection, and self-supervised learning. It provides a structured approach to defining each component, akin to writing your code with a clear framework.</p> <p>This structure offers powerful extensibility, allowing training experiments for classification and self-supervised learning to follow a consistent template. Below is an example of a System configuration for training a supervised classification model on the CIFAR10 dataset:</p> <pre><code>system:\n  _target_: lighter.System\n  batch_size: 512\n\n  model:\n    _target_: torchvision.models.resnet18\n    num_classes: 10\n\n  criterion:\n    _target_: torch.nn.CrossEntropyLoss\n\n  optimizer:\n    _target_: torch.optim.Adam\n    params: \"$@system#model.parameters()\"\n    lr: 0.001\n\n  datasets:\n    train:\n      _target_: torchvision.datasets.CIFAR10\n      download: True\n      root: .datasets\n      train: True\n      transform:\n        _target_: torchvision.transforms.Compose\n        transforms:\n          - _target_: torchvision.transforms.ToTensor\n          - _target_: torchvision.transforms.Normalize\n            mean: [0.5, 0.5, 0.5]\n            std: [0.5, 0.5, 0.5]\n      # Format the batch as required by Lighter.\n          - _target_: torchvision.transforms.Lambda\n            lambd: $lambda x: {\"input\": x[0], \"target\": x[1]}'\n</code></pre> <ol> <li>Postprocessing functions can be defined for various stages, such as batch processing, criterion evaluation, metrics calculation, and logging. These functions enable data modification at different points in the workflow, enhancing flexibility and control.</li> </ol> <p>For more information about each of the System components and how to override them, see here.</p>"},{"location":"basics/quickstart/#running-this-experiment-with-lighter","title":"Running this experiment with Lighter","text":"<p>To run an experiment with Lighter, combine the Trainer and System configurations into a single YAML file and execute the following command in your terminal:</p> cifar10.yamlTerminal <pre><code>trainer:\n  _target_: pytorch_lightning.Trainer\n  max_epochs: 100\n\nsystem:\n  _target_: lighter.System\n  batch_size: 512\n\n  model:\n    _target_: torchvision.models.resnet18\n    num_classes: 10\n\n  criterion:\n    _target_: torch.nn.CrossEntropyLoss\n\n  optimizer:\n    _target_: torch.optim.Adam\n    params: \"$@system#model.parameters()\"\n    lr: 0.001\n\n  datasets:\n    train:\n      _target_: torchvision.datasets.CIFAR10\n      download: True\n      root: .datasets\n      train: True\n      transform:\n        _target_: torchvision.transforms.Compose\n        transforms:\n          - _target_: torchvision.transforms.ToTensor\n          - _target_: torchvision.transforms.Normalize\n            mean: [0.5, 0.5, 0.5]\n            std: [0.5, 0.5, 0.5]\n\n  postprocessing:\n      # Ensure the batch is formatted as a dictionary with 'input' and 'target' keys.\n      batch:\n          train: '$lambda x: x'\n</code></pre> <pre><code>lighter fit --config cifar10.yaml\n</code></pre> <p>Congratulations! You've successfully run your first training example using Lighter.</p>"},{"location":"basics/workflows/","title":"Running Workflows","text":"<p>With your configuration in place, Lighter can execute various deep learning workflows. The supported workflows include:</p> <ol> <li>fit</li> <li>validate</li> <li>test</li> <li>predict</li> </ol> <p>These workflows are inherited from the PyTorch Lightning trainer and can be found in the PL docs.</p> <p>Below, we outline how to run these workflows and highlight the essential configuration elements required for each.</p>"},{"location":"basics/workflows/#fit-workflow","title":"Fit workflow","text":"<p>The fit workflow is designed for training models. Ensure that both the <code>trainer</code> and <code>system</code> configurations are specified in your YAML file.</p>"},{"location":"basics/workflows/#validate-workflow","title":"Validate workflow","text":"<p>The validate workflow evaluates models on a validation dataset. Make sure the <code>val</code> dataset is defined within the <code>system</code> configuration.</p>"},{"location":"basics/workflows/#test-workflow","title":"Test workflow","text":"<p>The test workflow assesses models using a test dataset. Confirm that the <code>test</code> dataset is included in the <code>system</code> configuration.</p>"},{"location":"basics/workflows/#predict-workflow","title":"Predict workflow","text":"<p>The predict workflow generates predictions on new data. Verify that the <code>predict</code> dataset is specified in the <code>system</code> configuration.</p>"},{"location":"reference/","title":"lighter","text":"<p>Lighter is a framework for streamlining deep learning experiments with configuration files.</p> <ul> <li>system</li> <li>callbacks</li> <li>utils</li> <li>engine</li> <li>adapters</li> </ul>"},{"location":"reference/SUMMARY/","title":"SUMMARY","text":"<ul> <li>lighter<ul> <li>adapters</li> <li>callbacks<ul> <li>freezer</li> <li>utils</li> <li>writer<ul> <li>base</li> <li>file</li> <li>table</li> </ul> </li> </ul> </li> <li>engine<ul> <li>config</li> <li>resolver</li> <li>runner</li> <li>schema</li> </ul> </li> <li>system</li> <li>utils<ul> <li>data</li> <li>dynamic_imports</li> <li>logging</li> <li>misc</li> <li>model</li> <li>types<ul> <li>containers</li> <li>enums</li> </ul> </li> </ul> </li> </ul> </li> </ul>"},{"location":"reference/adapters/","title":"adapters","text":""},{"location":"reference/adapters/#lighter.adapters.BatchAdapter","title":"<code>BatchAdapter</code>","text":"Source code in <code>lighter/adapters.py</code> <pre><code>class BatchAdapter:\n    def __init__(\n        self,\n        input_accessor: int | str | Callable,\n        target_accessor: int | str | Callable | None = None,\n        identifier_accessor: int | str | Callable | None = None,\n    ):\n        \"\"\"\n        Initializes BatchAdapter with accessors for input, target, and identifier.\n\n        Args:\n            input_accessor: Accessor for the identifier data. Can be an index (lists/tuples), a key (dictionaries),\n                a callable (custom batch processing).\n            target_accessor: Accessor for the target data. Can be an index (for lists/tuples),\n                             a key (for dictionaries), or a callable (for custom batch processing).\n            identifier_accessor: Accessor for the identifier data. Can be an index (lists/tuples), a key (dictionaries),\n                a callable (custom batch processing), or None if no identifier is present.\n        \"\"\"\n        self.input_accessor = input_accessor\n        self.target_accessor = target_accessor\n        self.identifier_accessor = identifier_accessor\n\n    def __call__(self, batch: Any) -&gt; tuple[Any, Any, Any]:\n        \"\"\"\n        Accesses the identifier, input, and target data from the batch.\n\n        Args:\n            batch: The batch data from which to extract information.\n\n        Returns:\n            A tuple containing (identifier, input, target).\n\n        Raises:\n            ValueError: If accessors are invalid for the provided batch structure.\n        \"\"\"\n        input = self._access_value(batch, self.input_accessor)\n        target = self._access_value(batch, self.target_accessor)\n        identifier = self._access_value(batch, self.identifier_accessor)\n        return input, target, identifier\n\n    def _access_value(self, data: Any, accessor: int | str | Callable) -&gt; Any:\n        \"\"\"\n        Accesses a value from the data using the provided accessor.\n\n        Args:\n            data: The data to access the value from.\n            accessor: The accessor to use. Can be an index (for lists/tuples),\n                      a key (for dictionaries), or a callable.\n\n        Returns:\n            The accessed value.\n\n        Raises:\n            ValueError: If the accessor type or data structure is invalid.\n        \"\"\"\n        if accessor is None:\n            return None\n        elif isinstance(accessor, int) and isinstance(data, (tuple, list)):\n            return data[accessor]\n        elif isinstance(accessor, str) and isinstance(data, dict):\n            return data[accessor]\n        elif callable(accessor):\n            return accessor(data)\n        else:\n            raise ValueError(f\"Invalid accessor {accessor} of type {type(accessor)} for data type {type(data)}.\")\n</code></pre>"},{"location":"reference/adapters/#lighter.adapters.BatchAdapter.__call__","title":"<code>__call__(batch)</code>","text":"<p>Accesses the identifier, input, and target data from the batch.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>Any</code> <p>The batch data from which to extract information.</p> required <p>Returns:</p> Type Description <code>tuple[Any, Any, Any]</code> <p>A tuple containing (identifier, input, target).</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If accessors are invalid for the provided batch structure.</p> Source code in <code>lighter/adapters.py</code> <pre><code>def __call__(self, batch: Any) -&gt; tuple[Any, Any, Any]:\n    \"\"\"\n    Accesses the identifier, input, and target data from the batch.\n\n    Args:\n        batch: The batch data from which to extract information.\n\n    Returns:\n        A tuple containing (identifier, input, target).\n\n    Raises:\n        ValueError: If accessors are invalid for the provided batch structure.\n    \"\"\"\n    input = self._access_value(batch, self.input_accessor)\n    target = self._access_value(batch, self.target_accessor)\n    identifier = self._access_value(batch, self.identifier_accessor)\n    return input, target, identifier\n</code></pre>"},{"location":"reference/adapters/#lighter.adapters.BatchAdapter.__init__","title":"<code>__init__(input_accessor, target_accessor=None, identifier_accessor=None)</code>","text":"<p>Initializes BatchAdapter with accessors for input, target, and identifier.</p> <p>Parameters:</p> Name Type Description Default <code>input_accessor</code> <code>int | str | Callable</code> <p>Accessor for the identifier data. Can be an index (lists/tuples), a key (dictionaries), a callable (custom batch processing).</p> required <code>target_accessor</code> <code>int | str | Callable | None</code> <p>Accessor for the target data. Can be an index (for lists/tuples),              a key (for dictionaries), or a callable (for custom batch processing).</p> <code>None</code> <code>identifier_accessor</code> <code>int | str | Callable | None</code> <p>Accessor for the identifier data. Can be an index (lists/tuples), a key (dictionaries), a callable (custom batch processing), or None if no identifier is present.</p> <code>None</code> Source code in <code>lighter/adapters.py</code> <pre><code>def __init__(\n    self,\n    input_accessor: int | str | Callable,\n    target_accessor: int | str | Callable | None = None,\n    identifier_accessor: int | str | Callable | None = None,\n):\n    \"\"\"\n    Initializes BatchAdapter with accessors for input, target, and identifier.\n\n    Args:\n        input_accessor: Accessor for the identifier data. Can be an index (lists/tuples), a key (dictionaries),\n            a callable (custom batch processing).\n        target_accessor: Accessor for the target data. Can be an index (for lists/tuples),\n                         a key (for dictionaries), or a callable (for custom batch processing).\n        identifier_accessor: Accessor for the identifier data. Can be an index (lists/tuples), a key (dictionaries),\n            a callable (custom batch processing), or None if no identifier is present.\n    \"\"\"\n    self.input_accessor = input_accessor\n    self.target_accessor = target_accessor\n    self.identifier_accessor = identifier_accessor\n</code></pre>"},{"location":"reference/adapters/#lighter.adapters.BatchAdapter._access_value","title":"<code>_access_value(data, accessor)</code>","text":"<p>Accesses a value from the data using the provided accessor.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Any</code> <p>The data to access the value from.</p> required <code>accessor</code> <code>int | str | Callable</code> <p>The accessor to use. Can be an index (for lists/tuples),       a key (for dictionaries), or a callable.</p> required <p>Returns:</p> Type Description <code>Any</code> <p>The accessed value.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the accessor type or data structure is invalid.</p> Source code in <code>lighter/adapters.py</code> <pre><code>def _access_value(self, data: Any, accessor: int | str | Callable) -&gt; Any:\n    \"\"\"\n    Accesses a value from the data using the provided accessor.\n\n    Args:\n        data: The data to access the value from.\n        accessor: The accessor to use. Can be an index (for lists/tuples),\n                  a key (for dictionaries), or a callable.\n\n    Returns:\n        The accessed value.\n\n    Raises:\n        ValueError: If the accessor type or data structure is invalid.\n    \"\"\"\n    if accessor is None:\n        return None\n    elif isinstance(accessor, int) and isinstance(data, (tuple, list)):\n        return data[accessor]\n    elif isinstance(accessor, str) and isinstance(data, dict):\n        return data[accessor]\n    elif callable(accessor):\n        return accessor(data)\n    else:\n        raise ValueError(f\"Invalid accessor {accessor} of type {type(accessor)} for data type {type(data)}.\")\n</code></pre>"},{"location":"reference/adapters/#lighter.adapters.CriterionAdapter","title":"<code>CriterionAdapter</code>","text":"<p>               Bases: <code>_ArgumentsAndTransformsAdapter</code></p> <p>This adapter processes and transforms the input, target, and prediction data, if specified, and forwards them to the specified arguments of a criterion (loss function).</p> Source code in <code>lighter/adapters.py</code> <pre><code>class CriterionAdapter(_ArgumentsAndTransformsAdapter):\n    \"\"\"\n    This adapter processes and transforms the input, target, and prediction data, if specified,\n    and forwards them to the specified arguments of a criterion (loss function).\n    \"\"\"\n\n    def __call__(self, criterion: Callable, input: Any, target: Any, pred: Any) -&gt; Any:\n        \"\"\"\n        Applies transforms and adapts arguments before calling the provided metric function.\n\n        Args:\n            crietion: The criterion (loss function).\n            input: The input data to transform with `input_transforms` if specified and pass to the metric with\n                the position or argument name specified by `input_argument`.\n            target: The target data to transform with `target_transforms` if specified and pass to the metric with\n                the position or argument name specified by `target_argument`.\n            pred: The prediction data to transform with `pred_transforms` if specified and pass to the metric with\n                the position or argument name specified by `pred_argument`.\n\n        Returns:\n            The result of the metric function call.\n        \"\"\"\n        return super().__call__(criterion, input, target, pred)\n</code></pre>"},{"location":"reference/adapters/#lighter.adapters.CriterionAdapter.__call__","title":"<code>__call__(criterion, input, target, pred)</code>","text":"<p>Applies transforms and adapts arguments before calling the provided metric function.</p> <p>Parameters:</p> Name Type Description Default <code>crietion</code> <p>The criterion (loss function).</p> required <code>input</code> <code>Any</code> <p>The input data to transform with <code>input_transforms</code> if specified and pass to the metric with the position or argument name specified by <code>input_argument</code>.</p> required <code>target</code> <code>Any</code> <p>The target data to transform with <code>target_transforms</code> if specified and pass to the metric with the position or argument name specified by <code>target_argument</code>.</p> required <code>pred</code> <code>Any</code> <p>The prediction data to transform with <code>pred_transforms</code> if specified and pass to the metric with the position or argument name specified by <code>pred_argument</code>.</p> required <p>Returns:</p> Type Description <code>Any</code> <p>The result of the metric function call.</p> Source code in <code>lighter/adapters.py</code> <pre><code>def __call__(self, criterion: Callable, input: Any, target: Any, pred: Any) -&gt; Any:\n    \"\"\"\n    Applies transforms and adapts arguments before calling the provided metric function.\n\n    Args:\n        crietion: The criterion (loss function).\n        input: The input data to transform with `input_transforms` if specified and pass to the metric with\n            the position or argument name specified by `input_argument`.\n        target: The target data to transform with `target_transforms` if specified and pass to the metric with\n            the position or argument name specified by `target_argument`.\n        pred: The prediction data to transform with `pred_transforms` if specified and pass to the metric with\n            the position or argument name specified by `pred_argument`.\n\n    Returns:\n        The result of the metric function call.\n    \"\"\"\n    return super().__call__(criterion, input, target, pred)\n</code></pre>"},{"location":"reference/adapters/#lighter.adapters.LoggingAdapter","title":"<code>LoggingAdapter</code>","text":"<p>               Bases: <code>_TransformsAdapter</code></p> <p>Adapter for applying logging transformations to data.</p> <p>This adapter handles the transformation of input, target, and prediction data specifically for logging purposes. It can preprocess or format the data before logging, ensuring consistency and readability in logs.</p> Source code in <code>lighter/adapters.py</code> <pre><code>class LoggingAdapter(_TransformsAdapter):\n    \"\"\"\n    Adapter for applying logging transformations to data.\n\n    This adapter handles the transformation of input, target, and prediction data\n    specifically for logging purposes. It can preprocess or format the data before\n    logging, ensuring consistency and readability in logs.\n\n    \"\"\"\n\n    def __init__(\n        self,\n        input_transforms: list[Callable] | None = None,\n        target_transforms: list[Callable] | None = None,\n        pred_transforms: list[Callable] | None = None,\n    ):\n        super().__init__(input_transforms, target_transforms, pred_transforms)\n</code></pre>"},{"location":"reference/adapters/#lighter.adapters.MetricsAdapter","title":"<code>MetricsAdapter</code>","text":"<p>               Bases: <code>_ArgumentsAndTransformsAdapter</code></p> <p>This adapter processes and transforms the input, target, and prediction data, if specified, and forwards them to the specified arguments of a metric.</p> Source code in <code>lighter/adapters.py</code> <pre><code>class MetricsAdapter(_ArgumentsAndTransformsAdapter):\n    \"\"\"\n    This adapter processes and transforms the input, target, and prediction data, if specified,\n    and forwards them to the specified arguments of a metric.\n    \"\"\"\n\n    def __call__(self, metric: Callable, input: Any, target: Any, pred: Any) -&gt; Any:\n        \"\"\"\n        Applies transforms and adapts arguments before calling the provided metric function.\n\n        Args:\n            metric: The metric.\n            input: The input data to transform with `input_transforms` if specified and pass to the metric with\n                the position or argument name specified by `input_argument`.\n            target: The target data to transform with `target_transforms` if specified and pass to the metric with\n                the position or argument name specified by `target_argument`.\n            pred: The prediction data to transform with `pred_transforms` if specified and pass to the metric with\n                the position or argument name specified by `pred_argument`.\n\n        Returns:\n            The result of the metric function call.\n        \"\"\"\n        return super().__call__(metric, input, target, pred)\n</code></pre>"},{"location":"reference/adapters/#lighter.adapters.MetricsAdapter.__call__","title":"<code>__call__(metric, input, target, pred)</code>","text":"<p>Applies transforms and adapts arguments before calling the provided metric function.</p> <p>Parameters:</p> Name Type Description Default <code>metric</code> <code>Callable</code> <p>The metric.</p> required <code>input</code> <code>Any</code> <p>The input data to transform with <code>input_transforms</code> if specified and pass to the metric with the position or argument name specified by <code>input_argument</code>.</p> required <code>target</code> <code>Any</code> <p>The target data to transform with <code>target_transforms</code> if specified and pass to the metric with the position or argument name specified by <code>target_argument</code>.</p> required <code>pred</code> <code>Any</code> <p>The prediction data to transform with <code>pred_transforms</code> if specified and pass to the metric with the position or argument name specified by <code>pred_argument</code>.</p> required <p>Returns:</p> Type Description <code>Any</code> <p>The result of the metric function call.</p> Source code in <code>lighter/adapters.py</code> <pre><code>def __call__(self, metric: Callable, input: Any, target: Any, pred: Any) -&gt; Any:\n    \"\"\"\n    Applies transforms and adapts arguments before calling the provided metric function.\n\n    Args:\n        metric: The metric.\n        input: The input data to transform with `input_transforms` if specified and pass to the metric with\n            the position or argument name specified by `input_argument`.\n        target: The target data to transform with `target_transforms` if specified and pass to the metric with\n            the position or argument name specified by `target_argument`.\n        pred: The prediction data to transform with `pred_transforms` if specified and pass to the metric with\n            the position or argument name specified by `pred_argument`.\n\n    Returns:\n        The result of the metric function call.\n    \"\"\"\n    return super().__call__(metric, input, target, pred)\n</code></pre>"},{"location":"reference/adapters/#lighter.adapters._ArgumentsAdapter","title":"<code>_ArgumentsAdapter</code>","text":"<p>Base adapter for adapting arguments to a function based on specified argument names or positions.</p> Source code in <code>lighter/adapters.py</code> <pre><code>class _ArgumentsAdapter:\n    \"\"\"\n    Base adapter for adapting arguments to a function based on specified argument names or positions.\n    \"\"\"\n\n    def __init__(\n        self,\n        input_argument: int | str | None = None,\n        target_argument: int | str | None = None,\n        pred_argument: int | str | None = None,\n    ):\n        # Ensure that the positionals are consecutive integers.\n        # There cannot be positional 0 and 2, without 1. Same with a positional 1 without 0.\n        positionals = sorted(arg for arg in (input_argument, target_argument, pred_argument) if isinstance(arg, int))\n        if positionals != list(range(len(positionals))):\n            raise ValueError(\"Positional arguments must be consecutive integers starting from 0.\")\n\n        self.input_argument = input_argument\n        self.target_argument = target_argument\n        self.pred_argument = pred_argument\n\n    def __call__(self, input: Any, target: Any, pred: Any) -&gt; tuple[list[Any], dict[str, Any]]:\n        \"\"\"\n        Adapts the input, target, and prediction data to the specified argument positions or names.\n\n        Args:\n            input: The input data to be adapted.\n            target: The target data to be adapted.\n            pred: The prediction data to be adapted.\n\n        Returns:\n            A tuple containing a list of positional arguments and a dictionary of keyword arguments.\n        \"\"\"\n        args = []  # List to store positional arguments\n        kwargs = {}  # Dictionary to store keyword arguments\n\n        # Mapping of argument names to their respective values\n        argument_map = {\"input_argument\": input, \"target_argument\": target, \"pred_argument\": pred}\n\n        # Iterate over the argument map to adapt arguments\n        for arg_name, value in argument_map.items():\n            # Get the position or name of the argument from the instance attributes\n            arg_position = getattr(self, arg_name)\n            if arg_position is not None:\n                if isinstance(arg_position, int):\n                    # Insert the value into the args list at the specified position\n                    args.insert(arg_position, value)\n                elif isinstance(arg_position, str):\n                    # Add the value to the kwargs dictionary with the specified name\n                    kwargs[arg_position] = value\n                else:\n                    # Raise an error if the argument type is invalid\n                    raise ValueError(f\"Invalid {arg_name} type: {type(arg_position)}\")\n\n        # Return the adapted positional and keyword arguments\n        return args, kwargs\n</code></pre>"},{"location":"reference/adapters/#lighter.adapters._ArgumentsAdapter.__call__","title":"<code>__call__(input, target, pred)</code>","text":"<p>Adapts the input, target, and prediction data to the specified argument positions or names.</p> <p>Parameters:</p> Name Type Description Default <code>input</code> <code>Any</code> <p>The input data to be adapted.</p> required <code>target</code> <code>Any</code> <p>The target data to be adapted.</p> required <code>pred</code> <code>Any</code> <p>The prediction data to be adapted.</p> required <p>Returns:</p> Type Description <code>tuple[list[Any], dict[str, Any]]</code> <p>A tuple containing a list of positional arguments and a dictionary of keyword arguments.</p> Source code in <code>lighter/adapters.py</code> <pre><code>def __call__(self, input: Any, target: Any, pred: Any) -&gt; tuple[list[Any], dict[str, Any]]:\n    \"\"\"\n    Adapts the input, target, and prediction data to the specified argument positions or names.\n\n    Args:\n        input: The input data to be adapted.\n        target: The target data to be adapted.\n        pred: The prediction data to be adapted.\n\n    Returns:\n        A tuple containing a list of positional arguments and a dictionary of keyword arguments.\n    \"\"\"\n    args = []  # List to store positional arguments\n    kwargs = {}  # Dictionary to store keyword arguments\n\n    # Mapping of argument names to their respective values\n    argument_map = {\"input_argument\": input, \"target_argument\": target, \"pred_argument\": pred}\n\n    # Iterate over the argument map to adapt arguments\n    for arg_name, value in argument_map.items():\n        # Get the position or name of the argument from the instance attributes\n        arg_position = getattr(self, arg_name)\n        if arg_position is not None:\n            if isinstance(arg_position, int):\n                # Insert the value into the args list at the specified position\n                args.insert(arg_position, value)\n            elif isinstance(arg_position, str):\n                # Add the value to the kwargs dictionary with the specified name\n                kwargs[arg_position] = value\n            else:\n                # Raise an error if the argument type is invalid\n                raise ValueError(f\"Invalid {arg_name} type: {type(arg_position)}\")\n\n    # Return the adapted positional and keyword arguments\n    return args, kwargs\n</code></pre>"},{"location":"reference/adapters/#lighter.adapters._ArgumentsAndTransformsAdapter","title":"<code>_ArgumentsAndTransformsAdapter</code>","text":"<p>               Bases: <code>_ArgumentsAdapter</code>, <code>_TransformsAdapter</code></p> <p>A generic adapter for applying functions (criterion or metrics) to data.</p> Source code in <code>lighter/adapters.py</code> <pre><code>class _ArgumentsAndTransformsAdapter(_ArgumentsAdapter, _TransformsAdapter):\n    \"\"\"\n    A generic adapter for applying functions (criterion or metrics) to data.\n    \"\"\"\n\n    def __init__(\n        self,\n        input_argument: int | str | None = None,\n        target_argument: int | str | None = None,\n        pred_argument: int | str | None = None,\n        input_transforms: list[Callable] | None = None,\n        target_transforms: list[Callable] | None = None,\n        pred_transforms: list[Callable] | None = None,\n    ):\n        \"\"\"\n        Initializes the Arguments and Transforms Adapter.\n\n        Args:\n            input_argument: Position or name for the input data.\n            target_argument: Position or name for the target data.\n            pred_argument: Position or name for the prediction data.\n            input_transforms: Transforms to apply to the input data.\n            target_transforms: Transforms to apply to the target data.\n            pred_transforms: Transforms to apply to the prediction data.\n\n        Raises:\n            ValueError: If transforms are provided without corresponding argument specifications.\n        \"\"\"\n        # Validate transform arguments\n        if input_argument is None and input_transforms is not None:\n            raise ValueError(\"Input transforms provided but input_argument is None\")\n        if target_argument is None and target_transforms is not None:\n            raise ValueError(\"Target transforms provided but target_argument is None\")\n        if pred_argument is None and pred_transforms is not None:\n            raise ValueError(\"Pred transforms provided but pred_argument is None\")\n\n        _ArgumentsAdapter.__init__(self, input_argument, target_argument, pred_argument)\n        _TransformsAdapter.__init__(self, input_transforms, target_transforms, pred_transforms)\n\n    def __call__(self, fn: Callable, input: Any, target: Any, pred: Any) -&gt; Any:\n        \"\"\"\n        Applies transforms and adapts arguments before calling the provided function.\n\n        Args:\n            fn: The function/method to be called (e.g., a loss function or metric).\n            input: The input data.\n            target: The target data.\n            pred: The prediction data.\n\n        Returns:\n            The result of the function call.\n        \"\"\"\n        # Apply the transforms to the input, target, and prediction data\n        input, target, pred = _TransformsAdapter.__call__(self, input, target, pred)\n        # Map the input, target, and prediction data to the function arguments\n        args, kwargs = _ArgumentsAdapter.__call__(self, input, target, pred)\n        # Call the provided function with the adapted arguments\n        return fn(*args, **kwargs)\n</code></pre>"},{"location":"reference/adapters/#lighter.adapters._ArgumentsAndTransformsAdapter.__call__","title":"<code>__call__(fn, input, target, pred)</code>","text":"<p>Applies transforms and adapts arguments before calling the provided function.</p> <p>Parameters:</p> Name Type Description Default <code>fn</code> <code>Callable</code> <p>The function/method to be called (e.g., a loss function or metric).</p> required <code>input</code> <code>Any</code> <p>The input data.</p> required <code>target</code> <code>Any</code> <p>The target data.</p> required <code>pred</code> <code>Any</code> <p>The prediction data.</p> required <p>Returns:</p> Type Description <code>Any</code> <p>The result of the function call.</p> Source code in <code>lighter/adapters.py</code> <pre><code>def __call__(self, fn: Callable, input: Any, target: Any, pred: Any) -&gt; Any:\n    \"\"\"\n    Applies transforms and adapts arguments before calling the provided function.\n\n    Args:\n        fn: The function/method to be called (e.g., a loss function or metric).\n        input: The input data.\n        target: The target data.\n        pred: The prediction data.\n\n    Returns:\n        The result of the function call.\n    \"\"\"\n    # Apply the transforms to the input, target, and prediction data\n    input, target, pred = _TransformsAdapter.__call__(self, input, target, pred)\n    # Map the input, target, and prediction data to the function arguments\n    args, kwargs = _ArgumentsAdapter.__call__(self, input, target, pred)\n    # Call the provided function with the adapted arguments\n    return fn(*args, **kwargs)\n</code></pre>"},{"location":"reference/adapters/#lighter.adapters._ArgumentsAndTransformsAdapter.__init__","title":"<code>__init__(input_argument=None, target_argument=None, pred_argument=None, input_transforms=None, target_transforms=None, pred_transforms=None)</code>","text":"<p>Initializes the Arguments and Transforms Adapter.</p> <p>Parameters:</p> Name Type Description Default <code>input_argument</code> <code>int | str | None</code> <p>Position or name for the input data.</p> <code>None</code> <code>target_argument</code> <code>int | str | None</code> <p>Position or name for the target data.</p> <code>None</code> <code>pred_argument</code> <code>int | str | None</code> <p>Position or name for the prediction data.</p> <code>None</code> <code>input_transforms</code> <code>list[Callable] | None</code> <p>Transforms to apply to the input data.</p> <code>None</code> <code>target_transforms</code> <code>list[Callable] | None</code> <p>Transforms to apply to the target data.</p> <code>None</code> <code>pred_transforms</code> <code>list[Callable] | None</code> <p>Transforms to apply to the prediction data.</p> <code>None</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If transforms are provided without corresponding argument specifications.</p> Source code in <code>lighter/adapters.py</code> <pre><code>def __init__(\n    self,\n    input_argument: int | str | None = None,\n    target_argument: int | str | None = None,\n    pred_argument: int | str | None = None,\n    input_transforms: list[Callable] | None = None,\n    target_transforms: list[Callable] | None = None,\n    pred_transforms: list[Callable] | None = None,\n):\n    \"\"\"\n    Initializes the Arguments and Transforms Adapter.\n\n    Args:\n        input_argument: Position or name for the input data.\n        target_argument: Position or name for the target data.\n        pred_argument: Position or name for the prediction data.\n        input_transforms: Transforms to apply to the input data.\n        target_transforms: Transforms to apply to the target data.\n        pred_transforms: Transforms to apply to the prediction data.\n\n    Raises:\n        ValueError: If transforms are provided without corresponding argument specifications.\n    \"\"\"\n    # Validate transform arguments\n    if input_argument is None and input_transforms is not None:\n        raise ValueError(\"Input transforms provided but input_argument is None\")\n    if target_argument is None and target_transforms is not None:\n        raise ValueError(\"Target transforms provided but target_argument is None\")\n    if pred_argument is None and pred_transforms is not None:\n        raise ValueError(\"Pred transforms provided but pred_argument is None\")\n\n    _ArgumentsAdapter.__init__(self, input_argument, target_argument, pred_argument)\n    _TransformsAdapter.__init__(self, input_transforms, target_transforms, pred_transforms)\n</code></pre>"},{"location":"reference/adapters/#lighter.adapters._TransformsAdapter","title":"<code>_TransformsAdapter</code>","text":"<p>Adapter for applying transformations to data.</p> <p>Parameters:</p> Name Type Description Default <code>input_transforms</code> <code>Callable | list[Callable] | None</code> <p>A single or a list of transforms to apply to the input data.</p> <code>None</code> <code>target_transforms</code> <code>Callable | list[Callable] | None</code> <p>A single or a list of transforms to apply to the target data.</p> <code>None</code> <code>pred_transforms</code> <code>Callable | list[Callable] | None</code> <p>A single or a list of transforms to apply to the prediction data.</p> <code>None</code> Source code in <code>lighter/adapters.py</code> <pre><code>class _TransformsAdapter:\n    \"\"\"\n    Adapter for applying transformations to data.\n\n    Args:\n        input_transforms: A single or a list of transforms to apply to the input data.\n        target_transforms: A single or a list of transforms to apply to the target data.\n        pred_transforms: A single or a list of transforms to apply to the prediction data.\n    \"\"\"\n\n    def __init__(\n        self,\n        input_transforms: Callable | list[Callable] | None = None,\n        target_transforms: Callable | list[Callable] | None = None,\n        pred_transforms: Callable | list[Callable] | None = None,\n    ):\n        self.input_transforms = input_transforms\n        self.target_transforms = target_transforms\n        self.pred_transforms = pred_transforms\n\n    def __call__(self, input: Any, target: Any, pred: Any) -&gt; tuple[Any, Any, Any]:\n        \"\"\"\n        Applies the specified transforms to the input, target, and prediction data.\n\n        Args:\n            input: The input data.\n            target: The target data.\n            pred: The prediction data.\n\n        Returns:\n            The transformed (input, target, prediction) data.\n        \"\"\"\n        input = self._transform(input, self.input_transforms)\n        target = self._transform(target, self.target_transforms)\n        pred = self._transform(pred, self.pred_transforms)\n        return input, target, pred\n\n    def _transform(self, data: Any, transforms: Callable | list[Callable]) -&gt; Any:\n        \"\"\"\n        Applies a list of transform functions to the data.\n\n        Args:\n            data: The data to be transformed.\n            transforms: A single transform function or a list of functions.\n\n        Returns:\n            The transformed data.\n\n        Raises:\n            ValueError: If any transform is not callable.\n        \"\"\"\n        for transform in ensure_list(transforms):\n            if callable(transform):\n                data = transform(data)\n            else:\n                raise ValueError(f\"Invalid transform type for transform: {transform}\")\n        return data\n</code></pre>"},{"location":"reference/adapters/#lighter.adapters._TransformsAdapter.__call__","title":"<code>__call__(input, target, pred)</code>","text":"<p>Applies the specified transforms to the input, target, and prediction data.</p> <p>Parameters:</p> Name Type Description Default <code>input</code> <code>Any</code> <p>The input data.</p> required <code>target</code> <code>Any</code> <p>The target data.</p> required <code>pred</code> <code>Any</code> <p>The prediction data.</p> required <p>Returns:</p> Type Description <code>tuple[Any, Any, Any]</code> <p>The transformed (input, target, prediction) data.</p> Source code in <code>lighter/adapters.py</code> <pre><code>def __call__(self, input: Any, target: Any, pred: Any) -&gt; tuple[Any, Any, Any]:\n    \"\"\"\n    Applies the specified transforms to the input, target, and prediction data.\n\n    Args:\n        input: The input data.\n        target: The target data.\n        pred: The prediction data.\n\n    Returns:\n        The transformed (input, target, prediction) data.\n    \"\"\"\n    input = self._transform(input, self.input_transforms)\n    target = self._transform(target, self.target_transforms)\n    pred = self._transform(pred, self.pred_transforms)\n    return input, target, pred\n</code></pre>"},{"location":"reference/adapters/#lighter.adapters._TransformsAdapter._transform","title":"<code>_transform(data, transforms)</code>","text":"<p>Applies a list of transform functions to the data.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Any</code> <p>The data to be transformed.</p> required <code>transforms</code> <code>Callable | list[Callable]</code> <p>A single transform function or a list of functions.</p> required <p>Returns:</p> Type Description <code>Any</code> <p>The transformed data.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If any transform is not callable.</p> Source code in <code>lighter/adapters.py</code> <pre><code>def _transform(self, data: Any, transforms: Callable | list[Callable]) -&gt; Any:\n    \"\"\"\n    Applies a list of transform functions to the data.\n\n    Args:\n        data: The data to be transformed.\n        transforms: A single transform function or a list of functions.\n\n    Returns:\n        The transformed data.\n\n    Raises:\n        ValueError: If any transform is not callable.\n    \"\"\"\n    for transform in ensure_list(transforms):\n        if callable(transform):\n            data = transform(data)\n        else:\n            raise ValueError(f\"Invalid transform type for transform: {transform}\")\n    return data\n</code></pre>"},{"location":"reference/system/","title":"system","text":"<p>This module defines the System class, which encapsulates the components of a deep learning system, including the model, optimizer, datasets, and more. It extends PyTorch Lightning's LightningModule.</p>"},{"location":"reference/system/#lighter.system.System","title":"<code>System</code>","text":"<p>               Bases: <code>LightningModule</code></p> <p>System encapsulates the components of a deep learning system, extending PyTorch Lightning's LightningModule.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Module</code> <p>Model.</p> required <code>optimizer</code> <code>Optimizer | None</code> <p>Optimizer.</p> <code>None</code> <code>scheduler</code> <code>LRScheduler | None</code> <p>Learning rate scheduler.</p> <code>None</code> <code>criterion</code> <code>Callable | None</code> <p>Criterion (loss) function.</p> <code>None</code> <code>metrics</code> <code>dict[str, Metric | list[Metric] | dict[str, Metric]] | None</code> <p>Metrics for train, val, and test. Supports a single/list/dict of <code>torchmetrics</code> metrics.</p> <code>None</code> <code>dataloaders</code> <code>dict[str, DataLoader]</code> <p>Dataloaders for train, val, test, and predict.</p> required <code>adapters</code> <code>dict[str, Callable] | None</code> <p>TODO</p> <code>None</code> <code>inferer</code> <code>Callable | None</code> <p>Inferer to use in val/test/predict modes. See MONAI inferers for more details: (https://docs.monai.io/en/stable/inferers.html).</p> <code>None</code> Source code in <code>lighter/system.py</code> <pre><code>class System(pl.LightningModule):\n    \"\"\"\n    System encapsulates the components of a deep learning system, extending PyTorch Lightning's LightningModule.\n\n    Args:\n        model: Model.\n        optimizer: Optimizer.\n        scheduler: Learning rate scheduler.\n        criterion: Criterion (loss) function.\n        metrics: Metrics for train, val, and test. Supports a single/list/dict of `torchmetrics` metrics.\n        dataloaders: Dataloaders for train, val, test, and predict.\n        adapters: TODO\n        inferer: Inferer to use in val/test/predict modes.\n            See MONAI inferers for more details: (https://docs.monai.io/en/stable/inferers.html).\n\n    \"\"\"\n\n    def __init__(\n        self,\n        model: Module,\n        dataloaders: dict[str, DataLoader],\n        optimizer: Optimizer | None = None,\n        scheduler: LRScheduler | None = None,\n        criterion: Callable | None = None,\n        metrics: dict[str, Metric | list[Metric] | dict[str, Metric]] | None = None,\n        adapters: dict[str, Callable] | None = None,\n        inferer: Callable | None = None,\n    ) -&gt; None:\n        super().__init__()\n\n        # Model setup\n        self.model = model\n        self.dataloaders = DataLoaders(**(dataloaders or {}))\n        self.optimizer = optimizer\n        self.scheduler = scheduler\n        self.criterion = criterion\n        self.metrics = Metrics(**(metrics or {}))\n        self.adapters = Adapters(**(adapters or {}))\n        self.inferer = inferer\n\n        self.mode = None\n        self._setup_mode_hooks()\n        self._register_metrics()\n\n    def _step(self, batch: dict, batch_idx: int) -&gt; dict[str, Any] | Any:\n        \"\"\"\n        Performs a step in the specified mode, processing the batch and calculating loss and metrics.\n\n        Args:\n            batch: The batch of data.\n            batch_idx: The index of the batch.\n        Returns:\n            dict or Any: For predict step, returns prediction only. For other steps,\n            returns dict with loss, metrics, input, target, pred, and identifier. Loss is None\n            for test step, metrics is None if unspecified.\n        \"\"\"\n        input, target, identifier = self._prepare_batch(batch)\n        pred = self.forward(input)\n\n        loss = self._calculate_loss(input, target, pred)\n        metrics = self._calculate_metrics(input, target, pred)\n\n        self._log_stats(loss, metrics, batch_idx)\n        output = self._prepare_output(identifier, input, target, pred, loss, metrics)\n        return output\n\n    def _prepare_batch(self, batch: dict) -&gt; tuple[Any, Any, Any]:\n        adapters = getattr(self.adapters, self.mode)\n        input, target, identifier = adapters.batch(batch)\n        return input, target, identifier\n\n    def forward(self, input: Any) -&gt; Any:  # pylint: disable=arguments-differ\n        \"\"\"\n        Forward pass through the model. Supports multi-input models.\n\n        Args:\n            input: The input data.\n\n        Returns:\n            Any: The model's output.\n        \"\"\"\n\n        # Pass `epoch` and/or `step` argument to forward if it accepts them\n        kwargs = {}\n        if hasarg(self.model.forward, Data.EPOCH):\n            kwargs[Data.EPOCH] = self.current_epoch\n        if hasarg(self.model.forward, Data.STEP):\n            kwargs[Data.STEP] = self.global_step\n\n        # Predict. Use inferer if available in val, test, and predict modes.\n        if self.inferer and self.mode in [Mode.VAL, Mode.TEST, Mode.PREDICT]:\n            return self.inferer(input, self.model, **kwargs)\n        return self.model(input, **kwargs)\n\n    def _calculate_loss(self, input: Any, target: Any, pred: Any) -&gt; Tensor | dict[str, Tensor] | None:\n        loss = None\n        if self.mode in [Mode.TRAIN, Mode.VAL]:\n            if self.criterion is None:\n                raise ValueError(\"Please specify 'system.criterion' in the config.\")\n\n            adapters = getattr(self.adapters, self.mode)\n            loss = adapters.criterion(self.criterion, input, target, pred)\n\n            if isinstance(loss, dict) and \"total\" not in loss:\n                raise ValueError(\n                    \"The loss dictionary must include a 'total' key that combines all sublosses. \"\n                    \"Example: {'total': combined_loss, 'subloss1': loss1, ...}\"\n                )\n        return loss\n\n    def _calculate_metrics(self, input: Any, target: Any, pred: Any) -&gt; Any | None:\n        metrics = None\n        if self.mode != Mode.PREDICT:\n            metrics = getattr(self.metrics, self.mode)\n            if metrics is not None:\n                adapters = getattr(self.adapters, self.mode)\n                metrics = adapters.metrics(metrics, input, target, pred)\n        return metrics\n\n    def _log_stats(self, loss: Tensor | dict[str, Tensor], metrics: MetricCollection, batch_idx: int) -&gt; None:\n        \"\"\"\n        Logs the loss, metrics, and optimizer statistics.\n\n        Args:\n            loss: The calculated loss.\n            metrics: The calculated metrics.\n            batch_idx: The index of the batch.\n        \"\"\"\n        if self.trainer.logger is None:\n            return\n\n        # Loss\n        if loss is not None:\n            if not isinstance(loss, dict):\n                self._log(f\"{self.mode}/{Data.LOSS}/{Data.STEP}\", loss, on_step=True)\n                self._log(f\"{self.mode}/{Data.LOSS}/{Data.EPOCH}\", loss, on_epoch=True)\n            else:\n                for name, subloss in loss.items():\n                    self._log(f\"{self.mode}/{Data.LOSS}/{name}/{Data.STEP}\", subloss, on_step=True)\n                    self._log(f\"{self.mode}/{Data.LOSS}/{name}/{Data.EPOCH}\", subloss, on_epoch=True)\n\n        # Metrics\n        if metrics is not None:\n            for name, metric in metrics.items():\n                self._log(f\"{self.mode}/{Data.METRICS}/{name}/{Data.STEP}\", metric, on_step=True)\n                self._log(f\"{self.mode}/{Data.METRICS}/{name}/{Data.EPOCH}\", metric, on_epoch=True)\n\n        # Optimizer's lr, momentum, beta. Logged in train mode and once per epoch.\n        if self.mode == Mode.TRAIN and batch_idx == 0:\n            for name, optimizer_stat in get_optimizer_stats(self.optimizer).items():\n                self._log(f\"{self.mode}/{name}\", optimizer_stat, on_epoch=True)\n\n    def _log(self, name: str, value: Any, on_step: bool = False, on_epoch: bool = False) -&gt; None:\n        \"\"\"Log a key, value pair. Syncs across distributed nodes if `on_epoch` is True.\n\n        Args:\n            name (str): key to log.\n            value (Any): value to log.\n            batch_size (int): batch size.\n            on_step (bool, optional): if True, logs on step.\n            on_epoch (bool, optional): if True, logs on epoch with sync_dist=True.\n        \"\"\"\n        batch_size = getattr(self.dataloaders, self.mode).batch_size\n        self.log(name, value, logger=True, batch_size=batch_size, on_step=on_step, on_epoch=on_epoch, sync_dist=on_epoch)\n\n    def _prepare_output(\n        self,\n        identifier: Any,\n        input: Any,\n        target: Any,\n        pred: Any,\n        loss: Tensor | dict[str, Tensor] | None,\n        metrics: Any | None,\n    ) -&gt; dict[str, Any]:\n        adapters = getattr(self.adapters, self.mode)\n        input, target, pred = adapters.logging(input, target, pred)\n        return {\n            Data.IDENTIFIER: identifier,\n            Data.INPUT: input,\n            Data.TARGET: target,\n            Data.PRED: pred,\n            Data.LOSS: loss,\n            Data.METRICS: metrics,\n            Data.STEP: self.global_step,\n            Data.EPOCH: self.current_epoch,\n        }\n\n    def configure_optimizers(self) -&gt; dict:\n        \"\"\"\n        Configures the optimizers and learning rate schedulers.\n\n        Returns:\n            dict: A dictionary containing the optimizer and scheduler.\n        \"\"\"\n        if self.optimizer is None:\n            raise ValueError(\"Please specify 'system.optimizer' in the config.\")\n        if self.scheduler is None:\n            return {\"optimizer\": self.optimizer}\n        else:\n            return {\"optimizer\": self.optimizer, \"lr_scheduler\": self.scheduler}\n\n    def _register_metrics(self):\n        # Register metrics to move them to the appropriate device. ModuleDict not used because 'train' is a reserved key.\n        for mode, metric in asdict(self.metrics).items():\n            if isinstance(metric, Module):\n                self.add_module(f\"{Data.METRICS}_{mode}\", metric)\n\n    def _setup_mode_hooks(self):\n        if self.dataloaders.train is not None:\n            self.training_step = self._step\n            self.train_dataloader = lambda: self.dataloaders.train\n            self.on_train_start = lambda: self._on_mode_start(Mode.TRAIN)\n            self.on_train_end = self._on_mode_end\n        if self.dataloaders.val is not None:\n            self.validation_step = self._step\n            self.val_dataloader = lambda: self.dataloaders.val\n            self.on_validation_start = lambda: self._on_mode_start(Mode.VAL)\n            self.on_validation_end = self._on_mode_end\n        if self.dataloaders.test is not None:\n            self.test_step = self._step\n            self.test_dataloader = lambda: self.dataloaders.test\n            self.on_test_start = lambda: self._on_mode_start(Mode.TEST)\n            self.on_test_end = self._on_mode_end\n        if self.dataloaders.predict is not None:\n            self.predict_step = self._step\n            self.predict_dataloader = lambda: self.dataloaders.predict\n            self.on_predict_start = lambda: self._on_mode_start(Mode.PREDICT)\n            self.on_predict_end = self._on_mode_end\n\n    def _on_mode_start(self, mode: str | None) -&gt; None:\n        self.mode = mode\n\n    def _on_mode_end(self) -&gt; None:\n        self.mode = None\n\n    @property\n    def learning_rate(self) -&gt; float:\n        \"\"\"\n        Gets the learning rate of the optimizer.\n\n        Returns:\n            float: The learning rate.\n        \"\"\"\n        if len(self.optimizer.param_groups) &gt; 1:\n            raise ValueError(\"The learning rate is not available when there are multiple optimizer parameter groups.\")\n        return self.optimizer.param_groups[0][\"lr\"]\n\n    @learning_rate.setter\n    def learning_rate(self, value: float) -&gt; None:\n        \"\"\"\n        Sets the learning rate of the optimizer.\n\n        Args:\n            value: The new learning rate.\n        \"\"\"\n        if len(self.optimizer.param_groups) &gt; 1:\n            raise ValueError(\"The learning rate is not available when there are multiple optimizer parameter groups.\")\n        self.optimizer.param_groups[0][\"lr\"] = value\n</code></pre>"},{"location":"reference/system/#lighter.system.System.learning_rate","title":"<code>learning_rate</code>  <code>property</code> <code>writable</code>","text":"<p>Gets the learning rate of the optimizer.</p> <p>Returns:</p> Name Type Description <code>float</code> <code>float</code> <p>The learning rate.</p>"},{"location":"reference/system/#lighter.system.System._log","title":"<code>_log(name, value, on_step=False, on_epoch=False)</code>","text":"<p>Log a key, value pair. Syncs across distributed nodes if <code>on_epoch</code> is True.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>key to log.</p> required <code>value</code> <code>Any</code> <p>value to log.</p> required <code>batch_size</code> <code>int</code> <p>batch size.</p> required <code>on_step</code> <code>bool</code> <p>if True, logs on step.</p> <code>False</code> <code>on_epoch</code> <code>bool</code> <p>if True, logs on epoch with sync_dist=True.</p> <code>False</code> Source code in <code>lighter/system.py</code> <pre><code>def _log(self, name: str, value: Any, on_step: bool = False, on_epoch: bool = False) -&gt; None:\n    \"\"\"Log a key, value pair. Syncs across distributed nodes if `on_epoch` is True.\n\n    Args:\n        name (str): key to log.\n        value (Any): value to log.\n        batch_size (int): batch size.\n        on_step (bool, optional): if True, logs on step.\n        on_epoch (bool, optional): if True, logs on epoch with sync_dist=True.\n    \"\"\"\n    batch_size = getattr(self.dataloaders, self.mode).batch_size\n    self.log(name, value, logger=True, batch_size=batch_size, on_step=on_step, on_epoch=on_epoch, sync_dist=on_epoch)\n</code></pre>"},{"location":"reference/system/#lighter.system.System._log_stats","title":"<code>_log_stats(loss, metrics, batch_idx)</code>","text":"<p>Logs the loss, metrics, and optimizer statistics.</p> <p>Parameters:</p> Name Type Description Default <code>loss</code> <code>Tensor | dict[str, Tensor]</code> <p>The calculated loss.</p> required <code>metrics</code> <code>MetricCollection</code> <p>The calculated metrics.</p> required <code>batch_idx</code> <code>int</code> <p>The index of the batch.</p> required Source code in <code>lighter/system.py</code> <pre><code>def _log_stats(self, loss: Tensor | dict[str, Tensor], metrics: MetricCollection, batch_idx: int) -&gt; None:\n    \"\"\"\n    Logs the loss, metrics, and optimizer statistics.\n\n    Args:\n        loss: The calculated loss.\n        metrics: The calculated metrics.\n        batch_idx: The index of the batch.\n    \"\"\"\n    if self.trainer.logger is None:\n        return\n\n    # Loss\n    if loss is not None:\n        if not isinstance(loss, dict):\n            self._log(f\"{self.mode}/{Data.LOSS}/{Data.STEP}\", loss, on_step=True)\n            self._log(f\"{self.mode}/{Data.LOSS}/{Data.EPOCH}\", loss, on_epoch=True)\n        else:\n            for name, subloss in loss.items():\n                self._log(f\"{self.mode}/{Data.LOSS}/{name}/{Data.STEP}\", subloss, on_step=True)\n                self._log(f\"{self.mode}/{Data.LOSS}/{name}/{Data.EPOCH}\", subloss, on_epoch=True)\n\n    # Metrics\n    if metrics is not None:\n        for name, metric in metrics.items():\n            self._log(f\"{self.mode}/{Data.METRICS}/{name}/{Data.STEP}\", metric, on_step=True)\n            self._log(f\"{self.mode}/{Data.METRICS}/{name}/{Data.EPOCH}\", metric, on_epoch=True)\n\n    # Optimizer's lr, momentum, beta. Logged in train mode and once per epoch.\n    if self.mode == Mode.TRAIN and batch_idx == 0:\n        for name, optimizer_stat in get_optimizer_stats(self.optimizer).items():\n            self._log(f\"{self.mode}/{name}\", optimizer_stat, on_epoch=True)\n</code></pre>"},{"location":"reference/system/#lighter.system.System._step","title":"<code>_step(batch, batch_idx)</code>","text":"<p>Performs a step in the specified mode, processing the batch and calculating loss and metrics.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>dict</code> <p>The batch of data.</p> required <code>batch_idx</code> <code>int</code> <p>The index of the batch.</p> required <p>Returns:     dict or Any: For predict step, returns prediction only. For other steps,     returns dict with loss, metrics, input, target, pred, and identifier. Loss is None     for test step, metrics is None if unspecified.</p> Source code in <code>lighter/system.py</code> <pre><code>def _step(self, batch: dict, batch_idx: int) -&gt; dict[str, Any] | Any:\n    \"\"\"\n    Performs a step in the specified mode, processing the batch and calculating loss and metrics.\n\n    Args:\n        batch: The batch of data.\n        batch_idx: The index of the batch.\n    Returns:\n        dict or Any: For predict step, returns prediction only. For other steps,\n        returns dict with loss, metrics, input, target, pred, and identifier. Loss is None\n        for test step, metrics is None if unspecified.\n    \"\"\"\n    input, target, identifier = self._prepare_batch(batch)\n    pred = self.forward(input)\n\n    loss = self._calculate_loss(input, target, pred)\n    metrics = self._calculate_metrics(input, target, pred)\n\n    self._log_stats(loss, metrics, batch_idx)\n    output = self._prepare_output(identifier, input, target, pred, loss, metrics)\n    return output\n</code></pre>"},{"location":"reference/system/#lighter.system.System.configure_optimizers","title":"<code>configure_optimizers()</code>","text":"<p>Configures the optimizers and learning rate schedulers.</p> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>A dictionary containing the optimizer and scheduler.</p> Source code in <code>lighter/system.py</code> <pre><code>def configure_optimizers(self) -&gt; dict:\n    \"\"\"\n    Configures the optimizers and learning rate schedulers.\n\n    Returns:\n        dict: A dictionary containing the optimizer and scheduler.\n    \"\"\"\n    if self.optimizer is None:\n        raise ValueError(\"Please specify 'system.optimizer' in the config.\")\n    if self.scheduler is None:\n        return {\"optimizer\": self.optimizer}\n    else:\n        return {\"optimizer\": self.optimizer, \"lr_scheduler\": self.scheduler}\n</code></pre>"},{"location":"reference/system/#lighter.system.System.forward","title":"<code>forward(input)</code>","text":"<p>Forward pass through the model. Supports multi-input models.</p> <p>Parameters:</p> Name Type Description Default <code>input</code> <code>Any</code> <p>The input data.</p> required <p>Returns:</p> Name Type Description <code>Any</code> <code>Any</code> <p>The model's output.</p> Source code in <code>lighter/system.py</code> <pre><code>def forward(self, input: Any) -&gt; Any:  # pylint: disable=arguments-differ\n    \"\"\"\n    Forward pass through the model. Supports multi-input models.\n\n    Args:\n        input: The input data.\n\n    Returns:\n        Any: The model's output.\n    \"\"\"\n\n    # Pass `epoch` and/or `step` argument to forward if it accepts them\n    kwargs = {}\n    if hasarg(self.model.forward, Data.EPOCH):\n        kwargs[Data.EPOCH] = self.current_epoch\n    if hasarg(self.model.forward, Data.STEP):\n        kwargs[Data.STEP] = self.global_step\n\n    # Predict. Use inferer if available in val, test, and predict modes.\n    if self.inferer and self.mode in [Mode.VAL, Mode.TEST, Mode.PREDICT]:\n        return self.inferer(input, self.model, **kwargs)\n    return self.model(input, **kwargs)\n</code></pre>"},{"location":"reference/callbacks/","title":"callbacks","text":"<ul> <li>freezer</li> <li>writer</li> <li>utils</li> </ul>"},{"location":"reference/callbacks/freezer/","title":"freezer","text":"<p>This module provides the Freezer callback, which allows freezing model parameters during training.</p>"},{"location":"reference/callbacks/freezer/#lighter.callbacks.freezer.Freezer","title":"<code>Freezer</code>","text":"<p>               Bases: <code>Callback</code></p> <p>Callback to freeze model parameters during training. Parameters can be frozen by exact name or prefix. Freezing can be applied indefinitely or until a specified step/epoch.</p> <p>Parameters:</p> Name Type Description Default <code>names</code> <code>str | List[str] | None</code> <p>Full names of parameters to freeze.</p> <code>None</code> <code>name_starts_with</code> <code>str | List[str] | None</code> <p>Prefixes of parameter names to freeze.</p> <code>None</code> <code>except_names</code> <code>str | List[str] | None</code> <p>Names of parameters to exclude from freezing.</p> <code>None</code> <code>except_name_starts_with</code> <code>str | List[str] | None</code> <p>Prefixes of parameter names to exclude from freezing.</p> <code>None</code> <code>until_step</code> <code>int | None</code> <p>Maximum step to freeze parameters until.</p> <code>None</code> <code>until_epoch</code> <code>int | None</code> <p>Maximum epoch to freeze parameters until.</p> <code>None</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If neither <code>names</code> nor <code>name_starts_with</code> are specified.</p> <code>ValueError</code> <p>If both <code>until_step</code> and <code>until_epoch</code> are specified.</p> Source code in <code>lighter/callbacks/freezer.py</code> <pre><code>class Freezer(Callback):\n    \"\"\"\n    Callback to freeze model parameters during training. Parameters can be frozen by exact name or prefix.\n    Freezing can be applied indefinitely or until a specified step/epoch.\n\n    Args:\n        names: Full names of parameters to freeze.\n        name_starts_with: Prefixes of parameter names to freeze.\n        except_names: Names of parameters to exclude from freezing.\n        except_name_starts_with: Prefixes of parameter names to exclude from freezing.\n        until_step: Maximum step to freeze parameters until.\n        until_epoch: Maximum epoch to freeze parameters until.\n\n    Raises:\n        ValueError: If neither `names` nor `name_starts_with` are specified.\n        ValueError: If both `until_step` and `until_epoch` are specified.\n\n    \"\"\"\n\n    def __init__(\n        self,\n        names: str | List[str] | None = None,\n        name_starts_with: str | List[str] | None = None,\n        except_names: str | List[str] | None = None,\n        except_name_starts_with: str | List[str] | None = None,\n        until_step: int | None = None,\n        until_epoch: int | None = None,\n    ) -&gt; None:\n        super().__init__()\n\n        if names is None and name_starts_with is None:\n            raise ValueError(\"At least one of `names` or `name_starts_with` must be specified.\")\n\n        if until_step is not None and until_epoch is not None:\n            raise ValueError(\"Only one of `until_step` or `until_epoch` can be specified.\")\n\n        self.names = ensure_list(names)\n        self.name_starts_with = ensure_list(name_starts_with)\n        self.except_names = ensure_list(except_names)\n        self.except_name_starts_with = ensure_list(except_name_starts_with)\n        self.until_step = until_step\n        self.until_epoch = until_epoch\n\n        self._frozen_state = False\n\n    def on_train_batch_start(self, trainer: Trainer, pl_module: System, batch: Any, batch_idx: int) -&gt; None:\n        \"\"\"\n        Called at the start of each training batch to potentially freeze parameters.\n\n        Args:\n            trainer: The trainer instance.\n            pl_module: The System instance.\n            batch: The current batch.\n            batch_idx: The index of the batch.\n        \"\"\"\n        self._on_batch_start(trainer, pl_module)\n\n    def on_validation_batch_start(\n        self, trainer: Trainer, pl_module: System, batch: Any, batch_idx: int, dataloader_idx: int = 0\n    ) -&gt; None:\n        self._on_batch_start(trainer, pl_module)\n\n    def on_test_batch_start(\n        self, trainer: Trainer, pl_module: System, batch: Any, batch_idx: int, dataloader_idx: int = 0\n    ) -&gt; None:\n        self._on_batch_start(trainer, pl_module)\n\n    def on_predict_batch_start(\n        self, trainer: Trainer, pl_module: System, batch: Any, batch_idx: int, dataloader_idx: int = 0\n    ) -&gt; None:\n        self._on_batch_start(trainer, pl_module)\n\n    def _on_batch_start(self, trainer: Trainer, pl_module: System) -&gt; None:\n        \"\"\"\n        Freezes or unfreezes model parameters based on the current step or epoch.\n\n        Args:\n            trainer: The trainer instance.\n            pl_module: The System instance.\n        \"\"\"\n        current_step = trainer.global_step\n        current_epoch = trainer.current_epoch\n\n        if self.until_step is not None and current_step &gt;= self.until_step:\n            if self._frozen_state:\n                logger.info(f\"Reached step {self.until_step} - unfreezing the previously frozen layers.\")\n                self._set_model_requires_grad(pl_module, True)\n            return\n\n        if self.until_epoch is not None and current_epoch &gt;= self.until_epoch:\n            if self._frozen_state:\n                logger.info(f\"Reached epoch {self.until_epoch} - unfreezing the previously frozen layers.\")\n                self._set_model_requires_grad(pl_module, True)\n            return\n\n        if not self._frozen_state:\n            self._set_model_requires_grad(pl_module, False)\n\n    def _set_model_requires_grad(self, model: Module | System, requires_grad: bool) -&gt; None:\n        \"\"\"\n        Sets the requires_grad attribute for model parameters, effectively freezing or unfreezing them.\n\n        Args:\n            model: The model whose parameters to modify.\n            requires_grad: Whether to allow gradients (unfreeze) or not (freeze).\n        \"\"\"\n        # If the model is a `System`, get the underlying PyTorch model.\n        if isinstance(model, System):\n            model = model.model\n\n        frozen_layers = []\n        # Freeze the specified parameters.\n        for name, param in model.named_parameters():\n            # Leave the excluded-from-freezing parameters trainable.\n            if self.except_names and name in self.except_names:\n                param.requires_grad = True\n                continue\n            if self.except_name_starts_with and any(name.startswith(prefix) for prefix in self.except_name_starts_with):\n                param.requires_grad = True\n                continue\n\n            # Freeze/unfreeze the specified parameters, based on the `requires_grad` argument.\n            if self.names and name in self.names:\n                param.requires_grad = requires_grad\n                frozen_layers.append(name)\n                continue\n            if self.name_starts_with and any(name.startswith(prefix) for prefix in self.name_starts_with):\n                param.requires_grad = requires_grad\n                frozen_layers.append(name)\n                continue\n\n            # Otherwise, leave the parameter trainable.\n            param.requires_grad = True\n\n        self._frozen_state = not requires_grad\n        # Log only when freezing the parameters.\n        if self._frozen_state:\n            logger.info(\n                f\"Setting requires_grad={requires_grad} the following layers\"\n                + (f\" until step {self.until_step}\" if self.until_step is not None else \"\")\n                + (f\" until epoch {self.until_epoch}\" if self.until_epoch is not None else \"\")\n                + f\": {frozen_layers}\"\n            )\n</code></pre>"},{"location":"reference/callbacks/freezer/#lighter.callbacks.freezer.Freezer._on_batch_start","title":"<code>_on_batch_start(trainer, pl_module)</code>","text":"<p>Freezes or unfreezes model parameters based on the current step or epoch.</p> <p>Parameters:</p> Name Type Description Default <code>trainer</code> <code>Trainer</code> <p>The trainer instance.</p> required <code>pl_module</code> <code>System</code> <p>The System instance.</p> required Source code in <code>lighter/callbacks/freezer.py</code> <pre><code>def _on_batch_start(self, trainer: Trainer, pl_module: System) -&gt; None:\n    \"\"\"\n    Freezes or unfreezes model parameters based on the current step or epoch.\n\n    Args:\n        trainer: The trainer instance.\n        pl_module: The System instance.\n    \"\"\"\n    current_step = trainer.global_step\n    current_epoch = trainer.current_epoch\n\n    if self.until_step is not None and current_step &gt;= self.until_step:\n        if self._frozen_state:\n            logger.info(f\"Reached step {self.until_step} - unfreezing the previously frozen layers.\")\n            self._set_model_requires_grad(pl_module, True)\n        return\n\n    if self.until_epoch is not None and current_epoch &gt;= self.until_epoch:\n        if self._frozen_state:\n            logger.info(f\"Reached epoch {self.until_epoch} - unfreezing the previously frozen layers.\")\n            self._set_model_requires_grad(pl_module, True)\n        return\n\n    if not self._frozen_state:\n        self._set_model_requires_grad(pl_module, False)\n</code></pre>"},{"location":"reference/callbacks/freezer/#lighter.callbacks.freezer.Freezer._set_model_requires_grad","title":"<code>_set_model_requires_grad(model, requires_grad)</code>","text":"<p>Sets the requires_grad attribute for model parameters, effectively freezing or unfreezing them.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Module | System</code> <p>The model whose parameters to modify.</p> required <code>requires_grad</code> <code>bool</code> <p>Whether to allow gradients (unfreeze) or not (freeze).</p> required Source code in <code>lighter/callbacks/freezer.py</code> <pre><code>def _set_model_requires_grad(self, model: Module | System, requires_grad: bool) -&gt; None:\n    \"\"\"\n    Sets the requires_grad attribute for model parameters, effectively freezing or unfreezing them.\n\n    Args:\n        model: The model whose parameters to modify.\n        requires_grad: Whether to allow gradients (unfreeze) or not (freeze).\n    \"\"\"\n    # If the model is a `System`, get the underlying PyTorch model.\n    if isinstance(model, System):\n        model = model.model\n\n    frozen_layers = []\n    # Freeze the specified parameters.\n    for name, param in model.named_parameters():\n        # Leave the excluded-from-freezing parameters trainable.\n        if self.except_names and name in self.except_names:\n            param.requires_grad = True\n            continue\n        if self.except_name_starts_with and any(name.startswith(prefix) for prefix in self.except_name_starts_with):\n            param.requires_grad = True\n            continue\n\n        # Freeze/unfreeze the specified parameters, based on the `requires_grad` argument.\n        if self.names and name in self.names:\n            param.requires_grad = requires_grad\n            frozen_layers.append(name)\n            continue\n        if self.name_starts_with and any(name.startswith(prefix) for prefix in self.name_starts_with):\n            param.requires_grad = requires_grad\n            frozen_layers.append(name)\n            continue\n\n        # Otherwise, leave the parameter trainable.\n        param.requires_grad = True\n\n    self._frozen_state = not requires_grad\n    # Log only when freezing the parameters.\n    if self._frozen_state:\n        logger.info(\n            f\"Setting requires_grad={requires_grad} the following layers\"\n            + (f\" until step {self.until_step}\" if self.until_step is not None else \"\")\n            + (f\" until epoch {self.until_epoch}\" if self.until_epoch is not None else \"\")\n            + f\": {frozen_layers}\"\n        )\n</code></pre>"},{"location":"reference/callbacks/freezer/#lighter.callbacks.freezer.Freezer.on_train_batch_start","title":"<code>on_train_batch_start(trainer, pl_module, batch, batch_idx)</code>","text":"<p>Called at the start of each training batch to potentially freeze parameters.</p> <p>Parameters:</p> Name Type Description Default <code>trainer</code> <code>Trainer</code> <p>The trainer instance.</p> required <code>pl_module</code> <code>System</code> <p>The System instance.</p> required <code>batch</code> <code>Any</code> <p>The current batch.</p> required <code>batch_idx</code> <code>int</code> <p>The index of the batch.</p> required Source code in <code>lighter/callbacks/freezer.py</code> <pre><code>def on_train_batch_start(self, trainer: Trainer, pl_module: System, batch: Any, batch_idx: int) -&gt; None:\n    \"\"\"\n    Called at the start of each training batch to potentially freeze parameters.\n\n    Args:\n        trainer: The trainer instance.\n        pl_module: The System instance.\n        batch: The current batch.\n        batch_idx: The index of the batch.\n    \"\"\"\n    self._on_batch_start(trainer, pl_module)\n</code></pre>"},{"location":"reference/callbacks/utils/","title":"utils","text":"<p>This module provides utility functions for callbacks, including mode conversion and image preprocessing.</p>"},{"location":"reference/callbacks/utils/#lighter.callbacks.utils.preprocess_image","title":"<code>preprocess_image(image)</code>","text":"<p>Preprocess image for logging. For multiple 2D images, creates a grid. For 3D images, stacks slices vertically. For multiple 3D images, creates a grid with each column showing a different 3D image stacked vertically.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>Tensor</code> <p>A 2D or 3D image tensor.</p> required <p>Returns:</p> Name Type Description <code>Tensor</code> <code>Tensor</code> <p>The preprocessed image ready for logging.</p> Source code in <code>lighter/callbacks/utils.py</code> <pre><code>def preprocess_image(image: Tensor) -&gt; Tensor:\n    \"\"\"\n    Preprocess image for logging. For multiple 2D images, creates a grid.\n    For 3D images, stacks slices vertically. For multiple 3D images, creates a grid\n    with each column showing a different 3D image stacked vertically.\n\n    Args:\n        image: A 2D or 3D image tensor.\n\n    Returns:\n        Tensor: The preprocessed image ready for logging.\n    \"\"\"\n    # If 3D (BCDHW), concat the images vertically and horizontally.\n    if image.ndim == 5:\n        shape = image.shape\n        # BCDHW -&gt; BC(D*H)W. Combine slices of a 3D images vertically into a single 2D image.\n        image = image.view(shape[0], shape[1], shape[2] * shape[3], shape[4])\n        # BCDHW -&gt; 1CDH(B*W). Concat images in the batch horizontally, and unsqueeze to add back the B dim.\n        image = torch.cat([*image], dim=-1).unsqueeze(0)\n    # If only one image in the batch, select it and return it. Same happens when the images are 3D as they\n    # are combined into a single image. `make_grid` is called when a batch of multiple 2D image is provided.\n    return image[0] if image.shape[0] == 1 else torchvision.utils.make_grid(image, nrow=8)\n</code></pre>"},{"location":"reference/callbacks/writer/","title":"writer","text":"<ul> <li>file</li> <li>table</li> <li>base</li> </ul>"},{"location":"reference/callbacks/writer/base/","title":"base","text":"<p>This module provides the base class for defining custom writers in Lighter, allowing predictions to be saved in various formats.</p>"},{"location":"reference/callbacks/writer/base/#lighter.callbacks.writer.base.BaseWriter","title":"<code>BaseWriter</code>","text":"<p>               Bases: <code>ABC</code>, <code>Callback</code></p> <p>Base class for defining custom Writer. It provides the structure to save predictions in various formats.</p> Subclasses should implement <p>1) <code>self.writers</code> attribute to specify the supported formats and their corresponding writer functions. 2) <code>self.write()</code> method to specify the saving strategy for a prediction.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str | Path</code> <p>Path for saving predictions.</p> required <code>writer</code> <code>str | Callable</code> <p>Writer function or name of a registered writer.</p> required Source code in <code>lighter/callbacks/writer/base.py</code> <pre><code>class BaseWriter(ABC, Callback):\n    \"\"\"\n    Base class for defining custom Writer. It provides the structure to save predictions in various formats.\n\n    Subclasses should implement:\n        1) `self.writers` attribute to specify the supported formats and their corresponding writer functions.\n        2) `self.write()` method to specify the saving strategy for a prediction.\n\n    Args:\n        path (str | Path): Path for saving predictions.\n        writer (str | Callable): Writer function or name of a registered writer.\n    \"\"\"\n\n    def __init__(self, path: str | Path, writer: str | Callable) -&gt; None:\n        self.path = Path(path)\n\n        # Check if the writer is a string and if it exists in the writers dictionary\n        if isinstance(writer, str):\n            if writer not in self.writers:\n                raise ValueError(f\"Writer for format {writer} does not exist. Available writers: {self.writers.keys()}.\")\n            self.writer = self.writers[writer]\n        else:\n            # If the writer is not a string, it is assumed to be a callable function\n            self.writer = writer\n\n        # Prediction counter. Used when IDs are not provided. Initialized in `self.setup()` based on the DDP rank.\n        self._pred_counter = None\n\n    @property\n    @abstractmethod\n    def writers(self) -&gt; dict[str, Callable]:\n        \"\"\"\n        Property to define the default writer functions.\n        \"\"\"\n\n    @abstractmethod\n    def write(self, tensor: Tensor, identifier: int) -&gt; None:\n        \"\"\"\n        Method to define how a tensor should be saved. The input tensor will be a single tensor without\n        the batch dimension.\n\n        For each supported format, there should be a corresponding writer function registered in `self.writers`\n        A specific writer function can be retrieved using `self.get_writer(self.format)`.\n\n        Args:\n            tensor (Tensor): Tensor, without the batch dimension, to be saved.\n            identifier (int): Identifier for the tensor, can be used for naming files or adding table records.\n        \"\"\"\n\n    def setup(self, trainer: Trainer, pl_module: System, stage: str) -&gt; None:\n        \"\"\"\n        Sets up the writer, ensuring the path is ready for saving predictions.\n\n        Args:\n            trainer (Trainer): The trainer instance.\n            pl_module (System): The System instance.\n            stage (str): The current stage of training.\n        \"\"\"\n        if stage != Stage.PREDICT:\n            return\n\n        # Initialize the prediction count with the rank of the current process\n        self._pred_counter = torch.distributed.get_rank() if trainer.world_size &gt; 1 else 0\n\n        # Ensure all distributed nodes write to the same path\n        self.path = trainer.strategy.broadcast(self.path, src=0)\n        directory = self.path.parent if self.path.suffix else self.path\n\n        # Warn if the path already exists\n        if self.path.exists():\n            logger.warning(f\"{self.path} already exists, existing predictions will be overwritten.\")\n\n        if trainer.is_global_zero:\n            directory.mkdir(parents=True, exist_ok=True)\n\n        # Wait for rank 0 to create the directory\n        trainer.strategy.barrier()\n\n        # Ensure all distributed nodes have access to the path\n        if not directory.exists():\n            raise RuntimeError(\n                f\"Rank {trainer.global_rank} does not share storage with rank 0. Ensure nodes have common storage access.\"\n            )\n\n    def on_predict_batch_end(\n        self, trainer: Trainer, pl_module: System, outputs: Any, batch: Any, batch_idx: int, dataloader_idx: int = 0\n    ) -&gt; None:\n        \"\"\"\n        Callback method executed at the end of each prediction batch to write predictions with unique IDs.\n\n        Args:\n            trainer (Trainer): The trainer instance.\n            pl_module (System): The System instance.\n            outputs (Any): The outputs from the prediction step.\n            batch (Any): The current batch.\n            batch_idx (int): The index of the batch.\n            dataloader_idx (int): The index of the dataloader.\n        \"\"\"\n        # If the IDs are not provided, generate global unique IDs based on the prediction count. DDP supported.\n        if outputs[Data.IDENTIFIER] is None:\n            batch_size = len(outputs[Data.PRED])\n            world_size = trainer.world_size\n            outputs[Data.IDENTIFIER] = list(\n                range(\n                    self._pred_counter,  # Start: counted globally, initialized with the rank of the current process\n                    self._pred_counter + batch_size * world_size,  # Stop: count the total batch size across all processes\n                    world_size,  # Step: each process writes predictions for every Nth sample\n                )\n            )\n            self._pred_counter += batch_size * world_size\n\n        for pred, identifier in zip(outputs[Data.PRED], outputs[Data.IDENTIFIER]):\n            self.write(tensor=pred, identifier=identifier)\n\n        # Clear the predictions to save CPU memory. https://github.com/Lightning-AI/pytorch-lightning/issues/19398\n        # pylint: disable=protected-access\n        trainer.predict_loop._predictions = [[] for _ in range(trainer.predict_loop.num_dataloaders)]\n        gc.collect()\n</code></pre>"},{"location":"reference/callbacks/writer/base/#lighter.callbacks.writer.base.BaseWriter.writers","title":"<code>writers</code>  <code>abstractmethod</code> <code>property</code>","text":"<p>Property to define the default writer functions.</p>"},{"location":"reference/callbacks/writer/base/#lighter.callbacks.writer.base.BaseWriter.on_predict_batch_end","title":"<code>on_predict_batch_end(trainer, pl_module, outputs, batch, batch_idx, dataloader_idx=0)</code>","text":"<p>Callback method executed at the end of each prediction batch to write predictions with unique IDs.</p> <p>Parameters:</p> Name Type Description Default <code>trainer</code> <code>Trainer</code> <p>The trainer instance.</p> required <code>pl_module</code> <code>System</code> <p>The System instance.</p> required <code>outputs</code> <code>Any</code> <p>The outputs from the prediction step.</p> required <code>batch</code> <code>Any</code> <p>The current batch.</p> required <code>batch_idx</code> <code>int</code> <p>The index of the batch.</p> required <code>dataloader_idx</code> <code>int</code> <p>The index of the dataloader.</p> <code>0</code> Source code in <code>lighter/callbacks/writer/base.py</code> <pre><code>def on_predict_batch_end(\n    self, trainer: Trainer, pl_module: System, outputs: Any, batch: Any, batch_idx: int, dataloader_idx: int = 0\n) -&gt; None:\n    \"\"\"\n    Callback method executed at the end of each prediction batch to write predictions with unique IDs.\n\n    Args:\n        trainer (Trainer): The trainer instance.\n        pl_module (System): The System instance.\n        outputs (Any): The outputs from the prediction step.\n        batch (Any): The current batch.\n        batch_idx (int): The index of the batch.\n        dataloader_idx (int): The index of the dataloader.\n    \"\"\"\n    # If the IDs are not provided, generate global unique IDs based on the prediction count. DDP supported.\n    if outputs[Data.IDENTIFIER] is None:\n        batch_size = len(outputs[Data.PRED])\n        world_size = trainer.world_size\n        outputs[Data.IDENTIFIER] = list(\n            range(\n                self._pred_counter,  # Start: counted globally, initialized with the rank of the current process\n                self._pred_counter + batch_size * world_size,  # Stop: count the total batch size across all processes\n                world_size,  # Step: each process writes predictions for every Nth sample\n            )\n        )\n        self._pred_counter += batch_size * world_size\n\n    for pred, identifier in zip(outputs[Data.PRED], outputs[Data.IDENTIFIER]):\n        self.write(tensor=pred, identifier=identifier)\n\n    # Clear the predictions to save CPU memory. https://github.com/Lightning-AI/pytorch-lightning/issues/19398\n    # pylint: disable=protected-access\n    trainer.predict_loop._predictions = [[] for _ in range(trainer.predict_loop.num_dataloaders)]\n    gc.collect()\n</code></pre>"},{"location":"reference/callbacks/writer/base/#lighter.callbacks.writer.base.BaseWriter.setup","title":"<code>setup(trainer, pl_module, stage)</code>","text":"<p>Sets up the writer, ensuring the path is ready for saving predictions.</p> <p>Parameters:</p> Name Type Description Default <code>trainer</code> <code>Trainer</code> <p>The trainer instance.</p> required <code>pl_module</code> <code>System</code> <p>The System instance.</p> required <code>stage</code> <code>str</code> <p>The current stage of training.</p> required Source code in <code>lighter/callbacks/writer/base.py</code> <pre><code>def setup(self, trainer: Trainer, pl_module: System, stage: str) -&gt; None:\n    \"\"\"\n    Sets up the writer, ensuring the path is ready for saving predictions.\n\n    Args:\n        trainer (Trainer): The trainer instance.\n        pl_module (System): The System instance.\n        stage (str): The current stage of training.\n    \"\"\"\n    if stage != Stage.PREDICT:\n        return\n\n    # Initialize the prediction count with the rank of the current process\n    self._pred_counter = torch.distributed.get_rank() if trainer.world_size &gt; 1 else 0\n\n    # Ensure all distributed nodes write to the same path\n    self.path = trainer.strategy.broadcast(self.path, src=0)\n    directory = self.path.parent if self.path.suffix else self.path\n\n    # Warn if the path already exists\n    if self.path.exists():\n        logger.warning(f\"{self.path} already exists, existing predictions will be overwritten.\")\n\n    if trainer.is_global_zero:\n        directory.mkdir(parents=True, exist_ok=True)\n\n    # Wait for rank 0 to create the directory\n    trainer.strategy.barrier()\n\n    # Ensure all distributed nodes have access to the path\n    if not directory.exists():\n        raise RuntimeError(\n            f\"Rank {trainer.global_rank} does not share storage with rank 0. Ensure nodes have common storage access.\"\n        )\n</code></pre>"},{"location":"reference/callbacks/writer/base/#lighter.callbacks.writer.base.BaseWriter.write","title":"<code>write(tensor, identifier)</code>  <code>abstractmethod</code>","text":"<p>Method to define how a tensor should be saved. The input tensor will be a single tensor without the batch dimension.</p> <p>For each supported format, there should be a corresponding writer function registered in <code>self.writers</code> A specific writer function can be retrieved using <code>self.get_writer(self.format)</code>.</p> <p>Parameters:</p> Name Type Description Default <code>tensor</code> <code>Tensor</code> <p>Tensor, without the batch dimension, to be saved.</p> required <code>identifier</code> <code>int</code> <p>Identifier for the tensor, can be used for naming files or adding table records.</p> required Source code in <code>lighter/callbacks/writer/base.py</code> <pre><code>@abstractmethod\ndef write(self, tensor: Tensor, identifier: int) -&gt; None:\n    \"\"\"\n    Method to define how a tensor should be saved. The input tensor will be a single tensor without\n    the batch dimension.\n\n    For each supported format, there should be a corresponding writer function registered in `self.writers`\n    A specific writer function can be retrieved using `self.get_writer(self.format)`.\n\n    Args:\n        tensor (Tensor): Tensor, without the batch dimension, to be saved.\n        identifier (int): Identifier for the tensor, can be used for naming files or adding table records.\n    \"\"\"\n</code></pre>"},{"location":"reference/callbacks/writer/file/","title":"file","text":"<p>This module provides the FileWriter class, which writes predictions to files in various formats.</p>"},{"location":"reference/callbacks/writer/file/#lighter.callbacks.writer.file.FileWriter","title":"<code>FileWriter</code>","text":"<p>               Bases: <code>BaseWriter</code></p> <p>Writer for saving predictions to files in various formats including tensors, images, videos, and ITK images. Custom writer functions can be provided to extend supported formats. Args:     path: Directory path where output files will be saved.     writer: Either a string specifying a built-in writer or a custom writer function.         Built-in writers:             - \"tensor\": Saves raw tensor data (.pt)             - \"image\": Saves as image file (.png)             - \"video\": Saves as video file             - \"itk_nrrd\": Saves as ITK NRRD file (.nrrd)             - \"itk_seg_nrrd\": Saves as ITK segmentation NRRD file (.seg.nrrd)             - \"itk_nifti\": Saves as ITK NIfTI file (.nii.gz)         Custom writers must:             - Accept (path, tensor) arguments             - Handle single tensor input (no batch dimension)             - Save output to the specified path</p> Source code in <code>lighter/callbacks/writer/file.py</code> <pre><code>class FileWriter(BaseWriter):\n    \"\"\"\n    Writer for saving predictions to files in various formats including tensors, images, videos, and ITK images.\n    Custom writer functions can be provided to extend supported formats.\n    Args:\n        path: Directory path where output files will be saved.\n        writer: Either a string specifying a built-in writer or a custom writer function.\n            Built-in writers:\n                - \"tensor\": Saves raw tensor data (.pt)\n                - \"image\": Saves as image file (.png)\n                - \"video\": Saves as video file\n                - \"itk_nrrd\": Saves as ITK NRRD file (.nrrd)\n                - \"itk_seg_nrrd\": Saves as ITK segmentation NRRD file (.seg.nrrd)\n                - \"itk_nifti\": Saves as ITK NIfTI file (.nii.gz)\n            Custom writers must:\n                - Accept (path, tensor) arguments\n                - Handle single tensor input (no batch dimension)\n                - Save output to the specified path\n    \"\"\"\n\n    @property\n    def writers(self) -&gt; dict[str, Callable]:\n        return {\n            \"tensor\": write_tensor,\n            \"image\": write_image,\n            \"video\": write_video,\n            \"itk_nrrd\": partial(write_itk_image, suffix=\".nrrd\"),\n            \"itk_seg_nrrd\": partial(write_itk_image, suffix=\".seg.nrrd\"),\n            \"itk_nifti\": partial(write_itk_image, suffix=\".nii.gz\"),\n        }\n\n    def write(self, tensor: Tensor, identifier: int | str) -&gt; None:\n        \"\"\"\n        Writes the tensor to a file using the specified writer.\n\n        Args:\n            tensor: The tensor to write.\n            identifier: Identifier for naming the file.\n        \"\"\"\n        if not self.path.is_dir():\n            raise RuntimeError(f\"FileWriter expects a directory path, got {self.path}\")\n\n        # Determine the path for the file based on prediction count. The suffix must be added by the writer function.\n        path = self.path / str(identifier)\n        # Write the tensor to the file.\n        self.writer(path, tensor)\n</code></pre>"},{"location":"reference/callbacks/writer/file/#lighter.callbacks.writer.file.FileWriter.write","title":"<code>write(tensor, identifier)</code>","text":"<p>Writes the tensor to a file using the specified writer.</p> <p>Parameters:</p> Name Type Description Default <code>tensor</code> <code>Tensor</code> <p>The tensor to write.</p> required <code>identifier</code> <code>int | str</code> <p>Identifier for naming the file.</p> required Source code in <code>lighter/callbacks/writer/file.py</code> <pre><code>def write(self, tensor: Tensor, identifier: int | str) -&gt; None:\n    \"\"\"\n    Writes the tensor to a file using the specified writer.\n\n    Args:\n        tensor: The tensor to write.\n        identifier: Identifier for naming the file.\n    \"\"\"\n    if not self.path.is_dir():\n        raise RuntimeError(f\"FileWriter expects a directory path, got {self.path}\")\n\n    # Determine the path for the file based on prediction count. The suffix must be added by the writer function.\n    path = self.path / str(identifier)\n    # Write the tensor to the file.\n    self.writer(path, tensor)\n</code></pre>"},{"location":"reference/callbacks/writer/file/#lighter.callbacks.writer.file.write_image","title":"<code>write_image(path, tensor)</code>","text":"<p>Writes a tensor as an image file in .png format.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <p>The path to save the image.</p> required <code>tensor</code> <p>The tensor representing the image.</p> required Source code in <code>lighter/callbacks/writer/file.py</code> <pre><code>def write_image(path, tensor):\n    \"\"\"\n    Writes a tensor as an image file in .png format.\n\n    Args:\n        path: The path to save the image.\n        tensor: The tensor representing the image.\n    \"\"\"\n    path = path.with_suffix(\".png\")\n    tensor = preprocess_image(tensor)\n    torchvision.io.write_png(tensor, str(path))\n</code></pre>"},{"location":"reference/callbacks/writer/file/#lighter.callbacks.writer.file.write_itk_image","title":"<code>write_itk_image(path, tensor, suffix)</code>","text":"<p>Writes a tensor as an ITK image file.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>The path to save the ITK image.</p> required <code>tensor</code> <code>MetaTensor</code> <p>The tensor representing the image. Must be in MONAI MetaTensor format.</p> required <code>suffix</code> <p>The file suffix indicating the format.</p> required Source code in <code>lighter/callbacks/writer/file.py</code> <pre><code>def write_itk_image(path: str, tensor: MetaTensor, suffix) -&gt; None:\n    \"\"\"\n    Writes a tensor as an ITK image file.\n\n    Args:\n        path: The path to save the ITK image.\n        tensor: The tensor representing the image. Must be in MONAI MetaTensor format.\n        suffix: The file suffix indicating the format.\n    \"\"\"\n    path = path.with_suffix(suffix)\n    if not isinstance(tensor, MetaTensor):\n        raise TypeError(\"Tensor must be in MONAI MetaTensor format.\")\n    itk_image = metatensor_to_itk_image(tensor, channel_dim=0, dtype=tensor.dtype)\n    OPTIONAL_IMPORTS[\"itk\"].imwrite(itk_image, str(path), True)\n</code></pre>"},{"location":"reference/callbacks/writer/file/#lighter.callbacks.writer.file.write_tensor","title":"<code>write_tensor(path, tensor)</code>","text":"<p>Writes a tensor to a file in .pt format.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <p>The path to save the tensor.</p> required <code>tensor</code> <p>The tensor to save.</p> required Source code in <code>lighter/callbacks/writer/file.py</code> <pre><code>def write_tensor(path, tensor):\n    \"\"\"\n    Writes a tensor to a file in .pt format.\n\n    Args:\n        path: The path to save the tensor.\n        tensor: The tensor to save.\n    \"\"\"\n    torch.save(tensor, path.with_suffix(\".pt\"))  # nosec B614\n</code></pre>"},{"location":"reference/callbacks/writer/file/#lighter.callbacks.writer.file.write_video","title":"<code>write_video(path, tensor)</code>","text":"<p>Writes a tensor as a video file in .mp4 format.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <p>The path to save the video.</p> required <code>tensor</code> <p>The tensor representing the video.</p> required Source code in <code>lighter/callbacks/writer/file.py</code> <pre><code>def write_video(path, tensor):\n    \"\"\"\n    Writes a tensor as a video file in .mp4 format.\n\n    Args:\n        path: The path to save the video.\n        tensor: The tensor representing the video.\n    \"\"\"\n    path = path.with_suffix(\".mp4\")\n    # Video tensor must be divisible by 2. Pad the height and width.\n    tensor = DivisiblePad(k=(0, 2, 2), mode=\"minimum\")(tensor)\n    # Video tensor must be THWC. Permute CTHW -&gt; THWC.\n    tensor = tensor.permute(1, 2, 3, 0)\n    # Video tensor must have 3 channels (RGB). Repeat the channel dim to convert grayscale to RGB.\n    if tensor.shape[-1] == 1:\n        tensor = tensor.repeat(1, 1, 1, 3)\n    # Video tensor must be in the range [0, 1]. Scale to [0, 255].\n    tensor = (tensor * 255).to(torch.uint8)\n    torchvision.io.write_video(str(path), tensor, fps=24)\n</code></pre>"},{"location":"reference/callbacks/writer/table/","title":"table","text":"<p>This module provides the TableWriter class, which saves predictions in a table format, such as CSV.</p>"},{"location":"reference/callbacks/writer/table/#lighter.callbacks.writer.table.TableWriter","title":"<code>TableWriter</code>","text":"<p>               Bases: <code>BaseWriter</code></p> <p>Writer for saving predictions in a table format, such as CSV.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str | Path</code> <p>CSV filepath.</p> required <code>writer</code> <code>str | Callable</code> <p>Writer function or name of a registered writer.</p> required Source code in <code>lighter/callbacks/writer/table.py</code> <pre><code>class TableWriter(BaseWriter):\n    \"\"\"\n    Writer for saving predictions in a table format, such as CSV.\n\n    Args:\n        path: CSV filepath.\n        writer: Writer function or name of a registered writer.\n    \"\"\"\n\n    def __init__(self, path: str | Path, writer: str | Callable) -&gt; None:\n        super().__init__(path, writer)\n        self.csv_records = []\n\n    @property\n    def writers(self) -&gt; dict[str, Callable]:\n        return {\n            \"tensor\": lambda tensor: tensor.item() if tensor.numel() == 1 else tensor.tolist(),\n        }\n\n    def write(self, tensor: Any, identifier: int | str) -&gt; None:\n        \"\"\"\n        Writes the tensor as a table record using the specified writer.\n\n        Args:\n            tensor: The tensor to record. Should not have a batch dimension.\n            identifier: Identifier for the record.\n        \"\"\"\n        self.csv_records.append({\"identifier\": identifier, \"pred\": self.writer(tensor)})\n\n    def on_predict_epoch_end(self, trainer: Trainer, pl_module: System) -&gt; None:\n        \"\"\"\n        Called at the end of the prediction epoch to save predictions to a CSV file.\n\n        Args:\n            trainer: The trainer instance.\n            pl_module: The System instance.\n        \"\"\"\n        # If in distributed data parallel mode, gather records from all processes to rank 0.\n        if trainer.world_size &gt; 1:\n            gather_csv_records = [None] * trainer.world_size if trainer.is_global_zero else None\n            torch.distributed.gather_object(self.csv_records, gather_csv_records, dst=0)\n            if trainer.is_global_zero:\n                self.csv_records = list(itertools.chain(*gather_csv_records))\n\n        # Save the records to a CSV file\n        if trainer.is_global_zero:\n            df = pd.DataFrame(self.csv_records)\n            try:\n                df = df.sort_values(\"identifier\")\n            except TypeError:\n                pass\n            df = df.set_index(\"identifier\")\n            df.to_csv(self.path)\n\n        # Clear the records after saving\n        self.csv_records = []\n</code></pre>"},{"location":"reference/callbacks/writer/table/#lighter.callbacks.writer.table.TableWriter.on_predict_epoch_end","title":"<code>on_predict_epoch_end(trainer, pl_module)</code>","text":"<p>Called at the end of the prediction epoch to save predictions to a CSV file.</p> <p>Parameters:</p> Name Type Description Default <code>trainer</code> <code>Trainer</code> <p>The trainer instance.</p> required <code>pl_module</code> <code>System</code> <p>The System instance.</p> required Source code in <code>lighter/callbacks/writer/table.py</code> <pre><code>def on_predict_epoch_end(self, trainer: Trainer, pl_module: System) -&gt; None:\n    \"\"\"\n    Called at the end of the prediction epoch to save predictions to a CSV file.\n\n    Args:\n        trainer: The trainer instance.\n        pl_module: The System instance.\n    \"\"\"\n    # If in distributed data parallel mode, gather records from all processes to rank 0.\n    if trainer.world_size &gt; 1:\n        gather_csv_records = [None] * trainer.world_size if trainer.is_global_zero else None\n        torch.distributed.gather_object(self.csv_records, gather_csv_records, dst=0)\n        if trainer.is_global_zero:\n            self.csv_records = list(itertools.chain(*gather_csv_records))\n\n    # Save the records to a CSV file\n    if trainer.is_global_zero:\n        df = pd.DataFrame(self.csv_records)\n        try:\n            df = df.sort_values(\"identifier\")\n        except TypeError:\n            pass\n        df = df.set_index(\"identifier\")\n        df.to_csv(self.path)\n\n    # Clear the records after saving\n    self.csv_records = []\n</code></pre>"},{"location":"reference/callbacks/writer/table/#lighter.callbacks.writer.table.TableWriter.write","title":"<code>write(tensor, identifier)</code>","text":"<p>Writes the tensor as a table record using the specified writer.</p> <p>Parameters:</p> Name Type Description Default <code>tensor</code> <code>Any</code> <p>The tensor to record. Should not have a batch dimension.</p> required <code>identifier</code> <code>int | str</code> <p>Identifier for the record.</p> required Source code in <code>lighter/callbacks/writer/table.py</code> <pre><code>def write(self, tensor: Any, identifier: int | str) -&gt; None:\n    \"\"\"\n    Writes the tensor as a table record using the specified writer.\n\n    Args:\n        tensor: The tensor to record. Should not have a batch dimension.\n        identifier: Identifier for the record.\n    \"\"\"\n    self.csv_records.append({\"identifier\": identifier, \"pred\": self.writer(tensor)})\n</code></pre>"},{"location":"reference/engine/","title":"engine","text":"<ul> <li>runner</li> <li>config</li> <li>resolver</li> <li>schema</li> </ul>"},{"location":"reference/engine/config/","title":"config","text":""},{"location":"reference/engine/config/#lighter.engine.config.Config","title":"<code>Config</code>","text":"<p>Handles loading, overriding, validating, and normalizing configurations.</p> Source code in <code>lighter/engine/config.py</code> <pre><code>class Config:\n    \"\"\"\n    Handles loading, overriding, validating, and normalizing configurations.\n    \"\"\"\n\n    def __init__(\n        self,\n        config: str | dict,\n        validate: bool,\n        **config_overrides: Any,\n    ):\n        \"\"\"\n        Initialize the Config object.\n\n        Args:\n            config: Path to a YAML configuration file or a dictionary containing the configuration.\n            validate: Whether to validate the configuration.\n            config_overrides: Keyword arguments to override values in the configuration file\n        \"\"\"\n        if not isinstance(config, (dict, str, type(None))):\n            raise ValueError(\"Invalid type for 'config'. Must be a dictionary or (comma-separated) path(s) to YAML file(s).\")\n\n        self._config_parser = ConfigParser(globals=False)\n        self._config_parser.read_config(config)\n        self._config_parser.parse()\n\n        # TODO: verify that switching from .update(config_overrides) to .set(value, name) is\n        # a valid approach. The latter allows creation of currently non-existent keys.\n        for name, value in config_overrides.items():\n            self._config_parser.set(value, name)\n\n        # Validate the configuration\n        if validate:\n            validator = cerberus.Validator(SCHEMA)\n            valid = validator.validate(self.get())\n            if not valid:\n                errors = format_validation_errors(validator.errors)\n                raise ConfigurationException(errors)\n\n    def get(self, key: str | None = None, default: Any = None) -&gt; Any:\n        \"\"\"Get raw content for the given key. If key is None, get the entire config.\"\"\"\n        return self._config_parser.config if key is None else self._config_parser.config.get(key, default)\n\n    def get_parsed_content(self, key: str | None = None, default: Any = None) -&gt; Any:\n        \"\"\"\n        Get the parsed content for the given key. If key is None, get the entire parsed config.\n        \"\"\"\n        return self._config_parser.get_parsed_content(key, default=default)\n</code></pre>"},{"location":"reference/engine/config/#lighter.engine.config.Config.__init__","title":"<code>__init__(config, validate, **config_overrides)</code>","text":"<p>Initialize the Config object.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>str | dict</code> <p>Path to a YAML configuration file or a dictionary containing the configuration.</p> required <code>validate</code> <code>bool</code> <p>Whether to validate the configuration.</p> required <code>config_overrides</code> <code>Any</code> <p>Keyword arguments to override values in the configuration file</p> <code>{}</code> Source code in <code>lighter/engine/config.py</code> <pre><code>def __init__(\n    self,\n    config: str | dict,\n    validate: bool,\n    **config_overrides: Any,\n):\n    \"\"\"\n    Initialize the Config object.\n\n    Args:\n        config: Path to a YAML configuration file or a dictionary containing the configuration.\n        validate: Whether to validate the configuration.\n        config_overrides: Keyword arguments to override values in the configuration file\n    \"\"\"\n    if not isinstance(config, (dict, str, type(None))):\n        raise ValueError(\"Invalid type for 'config'. Must be a dictionary or (comma-separated) path(s) to YAML file(s).\")\n\n    self._config_parser = ConfigParser(globals=False)\n    self._config_parser.read_config(config)\n    self._config_parser.parse()\n\n    # TODO: verify that switching from .update(config_overrides) to .set(value, name) is\n    # a valid approach. The latter allows creation of currently non-existent keys.\n    for name, value in config_overrides.items():\n        self._config_parser.set(value, name)\n\n    # Validate the configuration\n    if validate:\n        validator = cerberus.Validator(SCHEMA)\n        valid = validator.validate(self.get())\n        if not valid:\n            errors = format_validation_errors(validator.errors)\n            raise ConfigurationException(errors)\n</code></pre>"},{"location":"reference/engine/config/#lighter.engine.config.Config.get","title":"<code>get(key=None, default=None)</code>","text":"<p>Get raw content for the given key. If key is None, get the entire config.</p> Source code in <code>lighter/engine/config.py</code> <pre><code>def get(self, key: str | None = None, default: Any = None) -&gt; Any:\n    \"\"\"Get raw content for the given key. If key is None, get the entire config.\"\"\"\n    return self._config_parser.config if key is None else self._config_parser.config.get(key, default)\n</code></pre>"},{"location":"reference/engine/config/#lighter.engine.config.Config.get_parsed_content","title":"<code>get_parsed_content(key=None, default=None)</code>","text":"<p>Get the parsed content for the given key. If key is None, get the entire parsed config.</p> Source code in <code>lighter/engine/config.py</code> <pre><code>def get_parsed_content(self, key: str | None = None, default: Any = None) -&gt; Any:\n    \"\"\"\n    Get the parsed content for the given key. If key is None, get the entire parsed config.\n    \"\"\"\n    return self._config_parser.get_parsed_content(key, default=default)\n</code></pre>"},{"location":"reference/engine/config/#lighter.engine.config.ConfigurationException","title":"<code>ConfigurationException</code>","text":"<p>               Bases: <code>Exception</code></p> <p>Custom exception for validation errors.</p> Source code in <code>lighter/engine/config.py</code> <pre><code>class ConfigurationException(Exception):\n    \"\"\"Custom exception for validation errors.\"\"\"\n\n    def __init__(self, errors: str):\n        super().__init__(f\"Configuration validation failed:\\n{errors}\")\n</code></pre>"},{"location":"reference/engine/config/#lighter.engine.config.format_validation_errors","title":"<code>format_validation_errors(errors)</code>","text":"<p>Recursively format validation errors into a readable string.</p> Source code in <code>lighter/engine/config.py</code> <pre><code>def format_validation_errors(errors: dict) -&gt; str:\n    \"\"\"\n    Recursively format validation errors into a readable string.\n    \"\"\"\n    messages = []\n\n    def process_error(key, value, base_path=\"\"):\n        full_key = f\"{base_path}.{key}\" if base_path else key\n\n        if isinstance(value, dict):\n            for sub_key, sub_value in value.items():\n                process_error(sub_key, sub_value, full_key)\n        elif isinstance(value, list):\n            for item in value:\n                if isinstance(item, str):\n                    messages.append(f\"{full_key}: {item}\")\n                elif isinstance(item, dict):\n                    process_error(key, item, base_path)\n                else:\n                    messages.append(f\"{full_key}: {item}\")\n        else:\n            messages.append(f\"{full_key}: {value}\")\n\n    process_error(\"\", errors)\n    return \"\\n\".join(messages)\n</code></pre>"},{"location":"reference/engine/resolver/","title":"resolver","text":""},{"location":"reference/engine/resolver/#lighter.engine.resolver.Resolver","title":"<code>Resolver</code>","text":"<p>Resolves stage-specific configurations from the main configuration.</p> Source code in <code>lighter/engine/resolver.py</code> <pre><code>class Resolver:\n    \"\"\"\n    Resolves stage-specific configurations from the main configuration.\n    \"\"\"\n\n    STAGE_MODES = {\n        Stage.FIT: [Mode.TRAIN, Mode.VAL],\n        Stage.VALIDATE: [Mode.VAL],\n        Stage.TEST: [Mode.TEST],\n        Stage.PREDICT: [Mode.PREDICT],\n        Stage.LR_FIND: [Mode.TRAIN, Mode.VAL],\n        Stage.SCALE_BATCH_SIZE: [Mode.TRAIN, Mode.VAL],\n    }\n\n    def __init__(self, config: Config):\n        self.config = config\n\n    def get_stage_config(self, stage: str) -&gt; Config:\n        \"\"\"Get stage-specific configuration by filtering unused components.\"\"\"\n        if stage not in self.STAGE_MODES:\n            raise ValueError(f\"Invalid stage: {stage}. Allowed stages are {list(self.STAGE_MODES)}\")\n\n        stage_config = self.config.get().copy()\n        system_config = stage_config.get(\"system\", {})\n        dataloader_config = system_config.get(\"dataloaders\", {})\n        metrics_config = system_config.get(\"metrics\", {})\n\n        # Remove dataloaders not relevant to the current stage\n        for mode in set(dataloader_config) - set(self.STAGE_MODES[stage]):\n            dataloader_config.pop(mode, None)\n\n        # Remove metrics not relevant to the current stage\n        for mode in set(metrics_config) - set(self.STAGE_MODES[stage]):\n            metrics_config.pop(mode, None)\n\n        # Remove optimizer, scheduler, and criterion if not relevant to the current stage\n        if stage in [Stage.VALIDATE, Stage.TEST, Stage.PREDICT]:\n            if stage != Stage.VALIDATE:\n                system_config.pop(\"criterion\", None)\n            system_config.pop(\"optimizer\", None)\n            system_config.pop(\"scheduler\", None)\n\n        # Retain only relevant args for the current stage\n        if \"args\" in stage_config:\n            stage_config[\"args\"] = {stage: stage_config[\"args\"].get(stage, {})}\n\n        return Config(stage_config, validate=False)\n</code></pre>"},{"location":"reference/engine/resolver/#lighter.engine.resolver.Resolver.get_stage_config","title":"<code>get_stage_config(stage)</code>","text":"<p>Get stage-specific configuration by filtering unused components.</p> Source code in <code>lighter/engine/resolver.py</code> <pre><code>def get_stage_config(self, stage: str) -&gt; Config:\n    \"\"\"Get stage-specific configuration by filtering unused components.\"\"\"\n    if stage not in self.STAGE_MODES:\n        raise ValueError(f\"Invalid stage: {stage}. Allowed stages are {list(self.STAGE_MODES)}\")\n\n    stage_config = self.config.get().copy()\n    system_config = stage_config.get(\"system\", {})\n    dataloader_config = system_config.get(\"dataloaders\", {})\n    metrics_config = system_config.get(\"metrics\", {})\n\n    # Remove dataloaders not relevant to the current stage\n    for mode in set(dataloader_config) - set(self.STAGE_MODES[stage]):\n        dataloader_config.pop(mode, None)\n\n    # Remove metrics not relevant to the current stage\n    for mode in set(metrics_config) - set(self.STAGE_MODES[stage]):\n        metrics_config.pop(mode, None)\n\n    # Remove optimizer, scheduler, and criterion if not relevant to the current stage\n    if stage in [Stage.VALIDATE, Stage.TEST, Stage.PREDICT]:\n        if stage != Stage.VALIDATE:\n            system_config.pop(\"criterion\", None)\n        system_config.pop(\"optimizer\", None)\n        system_config.pop(\"scheduler\", None)\n\n    # Retain only relevant args for the current stage\n    if \"args\" in stage_config:\n        stage_config[\"args\"] = {stage: stage_config[\"args\"].get(stage, {})}\n\n    return Config(stage_config, validate=False)\n</code></pre>"},{"location":"reference/engine/runner/","title":"runner","text":""},{"location":"reference/engine/runner/#lighter.engine.runner.Runner","title":"<code>Runner</code>","text":"<p>Executes the specified stage using the validated and resolved configurations.</p> Source code in <code>lighter/engine/runner.py</code> <pre><code>class Runner:\n    \"\"\"\n    Executes the specified stage using the validated and resolved configurations.\n    \"\"\"\n\n    def __init__(self):\n        self.config = None\n        self.resolver = None\n        self.system = None\n        self.trainer = None\n        self.args = None\n\n    def run(self, stage: str, config: str | dict | None = None, **config_overrides: Any) -&gt; None:\n        \"\"\"Run the specified stage with the given configuration.\"\"\"\n        seed_everything()\n        self.config = Config(config, **config_overrides, validate=True)\n\n        # Resolves stage-specific configuration\n        self.resolver = Resolver(self.config)\n\n        # Setup stage\n        self._setup_stage(stage)\n\n        # Run stage\n        self._run_stage(stage)\n\n    def _run_stage(self, stage: str) -&gt; None:\n        \"\"\"Execute the specified stage (method) of the trainer.\"\"\"\n        if stage in [Stage.LR_FIND, Stage.SCALE_BATCH_SIZE]:\n            stage_method = getattr(Tuner(self.trainer), stage)\n        else:\n            stage_method = getattr(self.trainer, stage)\n        stage_method(self.system, **self.args)\n\n    def _setup_stage(self, stage: str) -&gt; None:\n        # Prune the configuration to the stage-specific components\n        stage_config = self.resolver.get_stage_config(stage)\n\n        # Import project module\n        project_path = stage_config.get(\"project\")\n        if project_path:\n            import_module_from_path(\"project\", project_path)\n\n        # Initialize system\n        self.system = stage_config.get_parsed_content(\"system\")\n        if not isinstance(self.system, System):\n            raise ValueError(\"'system' must be an instance of System\")\n\n        # Initialize trainer\n        self.trainer = stage_config.get_parsed_content(\"trainer\")\n        if not isinstance(self.trainer, Trainer):\n            raise ValueError(\"'trainer' must be an instance of Trainer\")\n\n        # Set up arguments for the stage\n        self.args = stage_config.get_parsed_content(f\"args#{stage}\", default={})\n\n        # Save config to system checkpoint and trainer logger\n        self._save_config()\n\n    def _save_config(self) -&gt; None:\n        \"\"\"Save config to system checkpoint and trainer logger.\"\"\"\n        if self.system:\n            self.system.save_hyperparameters(self.config.get())\n        if self.trainer and self.trainer.logger:\n            self.trainer.logger.log_hyperparams(self.config.get())\n</code></pre>"},{"location":"reference/engine/runner/#lighter.engine.runner.Runner._run_stage","title":"<code>_run_stage(stage)</code>","text":"<p>Execute the specified stage (method) of the trainer.</p> Source code in <code>lighter/engine/runner.py</code> <pre><code>def _run_stage(self, stage: str) -&gt; None:\n    \"\"\"Execute the specified stage (method) of the trainer.\"\"\"\n    if stage in [Stage.LR_FIND, Stage.SCALE_BATCH_SIZE]:\n        stage_method = getattr(Tuner(self.trainer), stage)\n    else:\n        stage_method = getattr(self.trainer, stage)\n    stage_method(self.system, **self.args)\n</code></pre>"},{"location":"reference/engine/runner/#lighter.engine.runner.Runner._save_config","title":"<code>_save_config()</code>","text":"<p>Save config to system checkpoint and trainer logger.</p> Source code in <code>lighter/engine/runner.py</code> <pre><code>def _save_config(self) -&gt; None:\n    \"\"\"Save config to system checkpoint and trainer logger.\"\"\"\n    if self.system:\n        self.system.save_hyperparameters(self.config.get())\n    if self.trainer and self.trainer.logger:\n        self.trainer.logger.log_hyperparams(self.config.get())\n</code></pre>"},{"location":"reference/engine/runner/#lighter.engine.runner.Runner.run","title":"<code>run(stage, config=None, **config_overrides)</code>","text":"<p>Run the specified stage with the given configuration.</p> Source code in <code>lighter/engine/runner.py</code> <pre><code>def run(self, stage: str, config: str | dict | None = None, **config_overrides: Any) -&gt; None:\n    \"\"\"Run the specified stage with the given configuration.\"\"\"\n    seed_everything()\n    self.config = Config(config, **config_overrides, validate=True)\n\n    # Resolves stage-specific configuration\n    self.resolver = Resolver(self.config)\n\n    # Setup stage\n    self._setup_stage(stage)\n\n    # Run stage\n    self._run_stage(stage)\n</code></pre>"},{"location":"reference/engine/schema/","title":"schema","text":"<p>Defines the schema for configuration validation in the Lighter framework.</p> <p>The schema ensures user configurations are correctly structured and typed. It includes: - <code>_meta_</code>: Metadata as a dictionary. - <code>_requires_</code>: Runs first, primarily to be used for imports. - <code>project</code>: Project name as a string. - <code>vars</code>: Variables as a dictionary. - <code>args</code>: Arguments to pass to Trainer stage methods like <code>fit</code>, <code>validate</code>, <code>test</code>. - <code>trainer</code>: Trainer setup. - <code>system</code>: System setup, encapsulates model, criterion, optimizer, scheduler, inferer, metrics, dataloaders, and adapters.</p> <p>Used by the <code>Config</code> class for validation.</p>"},{"location":"reference/utils/","title":"utils","text":"<ul> <li>logging</li> <li>misc</li> <li>dynamic_imports</li> <li>types</li> <li>model</li> <li>data</li> </ul>"},{"location":"reference/utils/data/","title":"data","text":""},{"location":"reference/utils/data/#lighter.utils.data.collate_replace_corrupted","title":"<code>collate_replace_corrupted(batch, dataset, default_collate_fn=None)</code>","text":"<p>Collate function to handle corrupted examples in a batch by replacing them with valid ones.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>Any</code> <p>The batch of data from the DataLoader.</p> required <code>dataset</code> <code>Dataset</code> <p>The dataset being used, which should return <code>None</code> for corrupted examples.</p> required <code>default_collate_fn</code> <code>Callable | None</code> <p>The default collate function to use once the batch is clean.</p> <code>None</code> <p>Returns:</p> Type Description <code>Any</code> <p>A batch with corrupted examples replaced by valid ones.</p> Source code in <code>lighter/utils/data.py</code> <pre><code>def collate_replace_corrupted(\n    batch: Any, dataset: torch.utils.data.Dataset, default_collate_fn: Callable | None = None\n) -&gt; Any:\n    \"\"\"\n    Collate function to handle corrupted examples in a batch by replacing them with valid ones.\n\n    Args:\n        batch: The batch of data from the DataLoader.\n        dataset: The dataset being used, which should return `None` for corrupted examples.\n        default_collate_fn: The default collate function to use once the batch is clean.\n\n    Returns:\n        A batch with corrupted examples replaced by valid ones.\n    \"\"\"\n    # Use `torch.utils.data.dataloader.default_collate` if no other default collate function is specified.\n    default_collate_fn = default_collate_fn if default_collate_fn is not None else default_collate\n    # Idea from https://stackoverflow.com/a/57882783\n    original_batch_len = len(batch)\n    # Filter out all the Nones (corrupted examples).\n    batch = list(filter(lambda x: x is not None, batch))\n    filtered_batch_len = len(batch)\n    # Num of corrupted examples.\n    num_corrupted = original_batch_len - filtered_batch_len\n    if num_corrupted &gt; 0:\n        # Replace a corrupted example with another randomly selected example.\n        batch.extend([dataset[random.randint(0, len(dataset) - 1)] for _ in range(num_corrupted)])\n        # Recursive call to replace the replacements if they are corrupted.\n        return collate_replace_corrupted(batch, dataset)\n    # Finally, when the whole batch is fine, apply the default collate function.\n    return default_collate_fn(batch)\n</code></pre>"},{"location":"reference/utils/dynamic_imports/","title":"dynamic_imports","text":"<p>This module provides utilities for dynamic imports, allowing optional imports and importing modules from paths.</p>"},{"location":"reference/utils/dynamic_imports/#lighter.utils.dynamic_imports.OptionalImports","title":"<code>OptionalImports</code>  <code>dataclass</code>","text":"<p>Handles optional imports, allowing modules to be imported only if they are available.</p> <p>Attributes:</p> Name Type Description <code>imports</code> <code>dict[str, object]</code> <p>A dictionary to store the imported modules.</p> Example <pre><code>from lighter.utils.dynamic_imports import OPTIONAL_IMPORTS\nwriter = OPTIONAL_IMPORTS[\"tensorboard\"].SummaryWriter()\n</code></pre> Source code in <code>lighter/utils/dynamic_imports.py</code> <pre><code>@dataclass\nclass OptionalImports:\n    \"\"\"\n    Handles optional imports, allowing modules to be imported only if they are available.\n\n    Attributes:\n        imports: A dictionary to store the imported modules.\n\n    Example:\n        ```\n        from lighter.utils.dynamic_imports import OPTIONAL_IMPORTS\n        writer = OPTIONAL_IMPORTS[\"tensorboard\"].SummaryWriter()\n        ```\n    \"\"\"\n\n    imports: dict[str, object] = field(default_factory=dict)\n\n    def __getitem__(self, module_name: str) -&gt; object:\n        \"\"\"\n        Get the imported module by name, importing it if necessary.\n\n        Args:\n            module_name: Name of the module to import.\n\n        Raises:\n            ImportError: If the module is not available.\n\n        Returns:\n            object: The imported module.\n        \"\"\"\n        \"\"\"Get the imported module by name.\n\n        Args:\n            module_name: Name of the module to import.\n\n        Raises:\n            ImportError: If the module is not available.\n\n        Returns:\n            Imported module.\n        \"\"\"\n        if module_name not in self.imports:\n            self.imports[module_name], module_available = optional_import(module_name)\n            if not module_available:\n                raise ImportError(f\"'{module_name}' is not available. Make sure that it is installed and spelled correctly.\")\n        return self.imports[module_name]\n</code></pre>"},{"location":"reference/utils/dynamic_imports/#lighter.utils.dynamic_imports.OptionalImports.__getitem__","title":"<code>__getitem__(module_name)</code>","text":"<p>Get the imported module by name, importing it if necessary.</p> <p>Parameters:</p> Name Type Description Default <code>module_name</code> <code>str</code> <p>Name of the module to import.</p> required <p>Raises:</p> Type Description <code>ImportError</code> <p>If the module is not available.</p> <p>Returns:</p> Name Type Description <code>object</code> <code>object</code> <p>The imported module.</p> Source code in <code>lighter/utils/dynamic_imports.py</code> <pre><code>def __getitem__(self, module_name: str) -&gt; object:\n    \"\"\"\n    Get the imported module by name, importing it if necessary.\n\n    Args:\n        module_name: Name of the module to import.\n\n    Raises:\n        ImportError: If the module is not available.\n\n    Returns:\n        object: The imported module.\n    \"\"\"\n    \"\"\"Get the imported module by name.\n\n    Args:\n        module_name: Name of the module to import.\n\n    Raises:\n        ImportError: If the module is not available.\n\n    Returns:\n        Imported module.\n    \"\"\"\n    if module_name not in self.imports:\n        self.imports[module_name], module_available = optional_import(module_name)\n        if not module_available:\n            raise ImportError(f\"'{module_name}' is not available. Make sure that it is installed and spelled correctly.\")\n    return self.imports[module_name]\n</code></pre>"},{"location":"reference/utils/dynamic_imports/#lighter.utils.dynamic_imports.import_module_from_path","title":"<code>import_module_from_path(module_name, module_path)</code>","text":"<p>Import a module from a given path and assign it a specified name.</p> <p>Parameters:</p> Name Type Description Default <code>module_name</code> <code>str</code> <p>Name to assign to the imported module.</p> required <code>module_path</code> <code>str</code> <p>Path to the module being imported.</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>If the module has already been imported.</p> <code>FileNotFoundError</code> <p>If the <code>__init__.py</code> file is not found in the module path.</p> Source code in <code>lighter/utils/dynamic_imports.py</code> <pre><code>def import_module_from_path(module_name: str, module_path: str) -&gt; None:\n    \"\"\"\n    Import a module from a given path and assign it a specified name.\n\n    Args:\n        module_name: Name to assign to the imported module.\n        module_path: Path to the module being imported.\n\n    Raises:\n        ValueError: If the module has already been imported.\n        FileNotFoundError: If the `__init__.py` file is not found in the module path.\n    \"\"\"\n    # Based on https://stackoverflow.com/a/41595552.\n\n    if module_name in sys.modules:\n        logger.warning(f\"{module_name} has already been imported as module.\")\n        return\n\n    module_path = Path(module_path).resolve() / \"__init__.py\"\n    if not module_path.is_file():\n        raise FileNotFoundError(f\"No `__init__.py` in `{module_path}`.\")\n    spec = importlib.util.spec_from_file_location(module_name, str(module_path))\n    module = importlib.util.module_from_spec(spec)\n    spec.loader.exec_module(module)\n    sys.modules[module_name] = module\n    logger.info(f\"Imported {module_path.parent} as module '{module_name}'.\")\n</code></pre>"},{"location":"reference/utils/logging/","title":"logging","text":"<p>Logging utilities for configuring and setting up custom logging using Loguru and Rich.</p> <p>This module provides functionality to set up visually appealing logs with custom formatting, traceback handling, and suppression of detailed logs from specified modules. It includes color mapping for different log levels and handlers to intercept and redirect logging to Loguru.</p>"},{"location":"reference/utils/logging/#lighter.utils.logging._setup_logging","title":"<code>_setup_logging()</code>","text":"<p>Configures custom logging using Loguru and Rich for visually appealing logs. Sets up traceback handling and suppression of specific modules. Must be run before importing anything else to set up the loggers correctly.</p> Source code in <code>lighter/utils/logging.py</code> <pre><code>def _setup_logging():\n    \"\"\"\n    Configures custom logging using Loguru and Rich for visually appealing logs.\n    Sets up traceback handling and suppression of specific modules.\n    Must be run before importing anything else to set up the loggers correctly.\n    \"\"\"\n    import inspect\n    import logging\n    import warnings\n\n    import rich.logging\n    import rich.traceback\n    from loguru import logger\n\n    def formatter(record: dict) -&gt; str:\n        \"\"\"Format log messages for better readability and clarity. Used to configure Loguru with a Rich handler.\"\"\"\n        lvl_name = record[\"level\"].name\n        lvl_color = LOGGING_COLOR_MAP.get(lvl_name, \"cyan\")\n        return (\n            \"[not bold green]{time:YYYY/MM/DD HH:mm:ss.SSS}[/not bold green]  |  {level.icon}  \"\n            f\"[{lvl_color}]{lvl_name:&lt;10}[/{lvl_color}]|  [{lvl_color}]{{message}}[/{lvl_color}]\"\n        )\n\n    class InterceptHandler(logging.Handler):\n        \"\"\"Handler to redirect other libraries' logging to Loguru. Taken from Loguru's documentation:\n        https://github.com/Delgan/loguru?tab=readme-ov-file#entirely-compatible-with-standard-logging\n        \"\"\"\n\n        def emit(self, record: logging.LogRecord) -&gt; None:\n            # Get corresponding Loguru level if it exists.\n            level: str | int\n            try:\n                level = logger.level(record.levelname).name\n            except ValueError:\n                level = record.levelno\n\n            # Find caller from where originated the logged message.\n            frame, depth = inspect.currentframe(), 0\n            while frame and (depth == 0 or frame.f_code.co_filename == logging.__file__):\n                frame = frame.f_back\n                depth += 1\n\n            logger.opt(depth=depth, exception=record.exc_info).log(level, record.getMessage())\n\n    # Intercept logging and redirect to Loguru. Must be called before importing other libraries to work.\n    logging.getLogger().handlers = [InterceptHandler()]\n\n    # Configure Rich traceback.\n    suppress = [importlib.import_module(name) for name in SUPPRESSED_MODULES]\n    rich.traceback.install(show_locals=False, width=120, suppress=suppress)\n    # Rich handler for Loguru. Time and level are handled by the formatter.\n    rich_handler = rich.logging.RichHandler(markup=True, show_time=False, show_level=False)\n    logger.configure(handlers=[{\"sink\": rich_handler, \"format\": formatter}])\n\n    # Capture the `warnings` standard module with Loguru\n    # https://loguru.readthedocs.io/en/stable/resources/recipes.html#capturing-standard-stdout-stderr-and-warnings\n    warnings.showwarning = lambda message, *args, **kwargs: logger.opt(depth=2).warning(message)\n</code></pre>"},{"location":"reference/utils/misc/","title":"misc","text":"<p>This module contains miscellaneous utility functions for handling lists, attributes, and function arguments.</p>"},{"location":"reference/utils/misc/#lighter.utils.misc.ensure_list","title":"<code>ensure_list(input)</code>","text":"<p>Ensures that the input is wrapped in a list. If the input is None, returns an empty list.</p> <p>Parameters:</p> Name Type Description Default <code>input</code> <code>Any</code> <p>The input to wrap in a list.</p> required <p>Returns:</p> Name Type Description <code>List</code> <code>List</code> <p>The input wrapped in a list, or an empty list if input is None.</p> Source code in <code>lighter/utils/misc.py</code> <pre><code>def ensure_list(input: Any) -&gt; List:\n    \"\"\"\n    Ensures that the input is wrapped in a list. If the input is None, returns an empty list.\n\n    Args:\n        input: The input to wrap in a list.\n\n    Returns:\n        List: The input wrapped in a list, or an empty list if input is None.\n    \"\"\"\n    if isinstance(input, list):\n        return input\n    if isinstance(input, tuple):\n        return list(input)\n    if input is None:\n        return []\n    return [input]\n</code></pre>"},{"location":"reference/utils/misc/#lighter.utils.misc.get_name","title":"<code>get_name(_callable, include_module_name=False)</code>","text":"<p>Retrieves the name of a callable, optionally including the module name.</p> <p>Parameters:</p> Name Type Description Default <code>_callable</code> <code>Callable</code> <p>The callable whose name to retrieve.</p> required <code>include_module_name</code> <code>bool</code> <p>Whether to include the module name in the result.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The name of the callable, optionally prefixed with the module name.</p> Source code in <code>lighter/utils/misc.py</code> <pre><code>def get_name(_callable: Callable, include_module_name: bool = False) -&gt; str:\n    \"\"\"\n    Retrieves the name of a callable, optionally including the module name.\n\n    Args:\n        _callable: The callable whose name to retrieve.\n        include_module_name: Whether to include the module name in the result.\n\n    Returns:\n        str: The name of the callable, optionally prefixed with the module name.\n    \"\"\"\n    # Get the name directly from the callable's __name__ attribute\n    name = getattr(_callable, \"__name__\", type(_callable).__name__)\n\n    if include_module_name:\n        # Get the module name directly from the callable's __module__ attribute\n        module = getattr(_callable, \"__module__\", type(_callable).__module__)\n        name = f\"{module}.{name}\"\n\n    return name\n</code></pre>"},{"location":"reference/utils/misc/#lighter.utils.misc.get_optimizer_stats","title":"<code>get_optimizer_stats(optimizer)</code>","text":"<p>Extract learning rates and momentum values from a PyTorch optimizer.</p> <p>Collects learning rate and momentum/beta values from each parameter group in the optimizer and returns them in a dictionary. Keys are formatted to show the optimizer type and group number (if multiple groups exist).</p> <p>Parameters:</p> Name Type Description Default <code>optimizer</code> <code>Optimizer</code> <p>The PyTorch optimizer to extract values from.</p> required <p>Returns:</p> Type Description <code>dict[str, float]</code> <p>dict[str, float]: dictionary containing: - Learning rates: \"optimizer/{name}/lr[/group{N}]\" - Momentum values: \"optimizer/{name}/momentum[/group{N}]\"</p> <p>Where [/group{N}] is only added for optimizers with multiple groups.</p> Source code in <code>lighter/utils/misc.py</code> <pre><code>def get_optimizer_stats(optimizer: Optimizer) -&gt; dict[str, float]:\n    \"\"\"\n    Extract learning rates and momentum values from a PyTorch optimizer.\n\n    Collects learning rate and momentum/beta values from each parameter group\n    in the optimizer and returns them in a dictionary. Keys are formatted to show\n    the optimizer type and group number (if multiple groups exist).\n\n    Args:\n        optimizer: The PyTorch optimizer to extract values from.\n\n    Returns:\n        dict[str, float]: dictionary containing:\n            - Learning rates: \"optimizer/{name}/lr[/group{N}]\"\n            - Momentum values: \"optimizer/{name}/momentum[/group{N}]\"\n\n            Where [/group{N}] is only added for optimizers with multiple groups.\n    \"\"\"\n    stats_dict = {}\n    for group_idx, group in enumerate(optimizer.param_groups):\n        lr_key = f\"optimizer/{optimizer.__class__.__name__}/lr\"\n        momentum_key = f\"optimizer/{optimizer.__class__.__name__}/momentum\"\n\n        # Add group index to the key if there are multiple parameter groups\n        if len(optimizer.param_groups) &gt; 1:\n            lr_key += f\"/group{group_idx+1}\"\n            momentum_key += f\"/group{group_idx+1}\"\n\n        # Extracting learning rate\n        stats_dict[lr_key] = group[\"lr\"]\n\n        # Extracting momentum or betas[0] if available\n        if \"momentum\" in group:\n            stats_dict[momentum_key] = group[\"momentum\"]\n        if \"betas\" in group:\n            stats_dict[momentum_key] = group[\"betas\"][0]\n\n    return stats_dict\n</code></pre>"},{"location":"reference/utils/misc/#lighter.utils.misc.hasarg","title":"<code>hasarg(fn, arg_name)</code>","text":"<p>Checks if a callable (function, method, or class) has a specific argument.</p> <p>Parameters:</p> Name Type Description Default <code>fn</code> <code>Callable</code> <p>The callable to inspect.</p> required <code>arg_name</code> <code>str</code> <p>The name of the argument to check for.</p> required <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>True if the argument exists, False otherwise.</p> Source code in <code>lighter/utils/misc.py</code> <pre><code>def hasarg(fn: Callable, arg_name: str) -&gt; bool:\n    \"\"\"\n    Checks if a callable (function, method, or class) has a specific argument.\n\n    Args:\n        fn: The callable to inspect.\n        arg_name: The name of the argument to check for.\n\n    Returns:\n        bool: True if the argument exists, False otherwise.\n    \"\"\"\n    args = inspect.signature(fn).parameters.keys()\n    return arg_name in args\n</code></pre>"},{"location":"reference/utils/misc/#lighter.utils.misc.setattr_dot_notation","title":"<code>setattr_dot_notation(obj, attr, value)</code>","text":"<p>Sets an attribute on an object using dot notation.</p> <p>Parameters:</p> Name Type Description Default <code>obj</code> <code>Callable</code> <p>The object on which to set the attribute.</p> required <code>attr</code> <code>str</code> <p>The attribute name, which can use dot notation for nested attributes.</p> required <code>value</code> <code>Any</code> <p>The value to set the attribute to.</p> required Source code in <code>lighter/utils/misc.py</code> <pre><code>def setattr_dot_notation(obj: Callable, attr: str, value: Any) -&gt; None:\n    \"\"\"\n    Sets an attribute on an object using dot notation.\n\n    Args:\n        obj: The object on which to set the attribute.\n        attr: The attribute name, which can use dot notation for nested attributes.\n        value: The value to set the attribute to.\n    \"\"\"\n    if \".\" not in attr:\n        if not hasattr(obj, attr):\n            raise AttributeError(f\"`{get_name(obj, True)}` has no attribute `{attr}`.\")\n        setattr(obj, attr, value)\n    # Solve recursively if the attribute is defined in dot-notation\n    else:\n        obj_name, attr = attr.split(\".\", maxsplit=1)\n        setattr_dot_notation(getattr(obj, obj_name), attr, value)\n</code></pre>"},{"location":"reference/utils/model/","title":"model","text":"<p>This module provides utility functions for manipulating PyTorch models, such as replacing layers or loading state_dicts.</p>"},{"location":"reference/utils/model/#lighter.utils.model.adjust_prefix_and_load_state_dict","title":"<code>adjust_prefix_and_load_state_dict(model, ckpt_path, ckpt_to_model_prefix=None, layers_to_ignore=None)</code>","text":"<p>This function loads a state dictionary from a checkpoint file into a model using <code>torch.load(strict=False)</code>. It supports remapping layer names between the checkpoint and model through the <code>ckpt_to_model_prefix</code> parameter.</p> <p>This is useful when loading weights from a model that was trained as part of a larger architecture, where the layer names may not match the standalone version of the model.</p> <p>Before using <code>ckpt_to_model_prefix</code>, it's recommended to: 1. Check the layer names in both the checkpoint and target model 2. Map the mismatched prefixes accordingly</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Module</code> <p>The model to load the state_dict into.</p> required <code>ckpt_path</code> <code>str</code> <p>The path to the checkpoint file.</p> required <code>ckpt_to_model_prefix</code> <code>dict[str, str] | None</code> <p>Mapping of checkpoint prefixes to model prefixes.</p> <code>None</code> <code>layers_to_ignore</code> <code>List[str] | None</code> <p>Layers to ignore when loading the state_dict.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>Module</code> <code>Module</code> <p>The model with the loaded state_dict.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If there is no overlap between the checkpoint's and model's state_dict.</p> Source code in <code>lighter/utils/model.py</code> <pre><code>def adjust_prefix_and_load_state_dict(\n    model: Module,\n    ckpt_path: str,\n    ckpt_to_model_prefix: dict[str, str] | None = None,\n    layers_to_ignore: List[str] | None = None,\n) -&gt; Module:\n    \"\"\"\n    This function loads a state dictionary from a checkpoint file into a model using `torch.load(strict=False)`.\n    It supports remapping layer names between the checkpoint and model through the `ckpt_to_model_prefix` parameter.\n\n    This is useful when loading weights from a model that was trained as part of a larger architecture,\n    where the layer names may not match the standalone version of the model.\n\n    Before using `ckpt_to_model_prefix`, it's recommended to:\n    1. Check the layer names in both the checkpoint and target model\n    2. Map the mismatched prefixes accordingly\n\n    Args:\n        model: The model to load the state_dict into.\n        ckpt_path: The path to the checkpoint file.\n        ckpt_to_model_prefix: Mapping of checkpoint prefixes to model prefixes.\n        layers_to_ignore: Layers to ignore when loading the state_dict.\n\n    Returns:\n        Module: The model with the loaded state_dict.\n\n    Raises:\n        ValueError: If there is no overlap between the checkpoint's and model's state_dict.\n    \"\"\"\n    # Load checkpoint and handle if state_dict is nested.\n    ckpt = torch.load(ckpt_path)  # nosec B614\n    if \"state_dict\" in ckpt:\n        # System has a model attribute that contains the actual model, remove the \"model.\" prefix\n        ckpt = {key.replace(\"model.\", \"\"): value for key, value in ckpt[\"state_dict\"].items()}\n\n    # Adjust checkpoint keys based on prefix mapping\n    adjusted_ckpt = {}\n    if ckpt_to_model_prefix:\n        for ckpt_prefix, model_prefix in ckpt_to_model_prefix.items():\n            ckpt_prefix = f\"{ckpt_prefix}.\" if ckpt_prefix and not ckpt_prefix.endswith(\".\") else ckpt_prefix\n            model_prefix = f\"{model_prefix}.\" if model_prefix and not model_prefix.endswith(\".\") else model_prefix\n\n            if ckpt_prefix:\n                adjusted_ckpt.update(\n                    {key.replace(ckpt_prefix, model_prefix): value for key, value in ckpt.items() if ckpt_prefix in key}\n                )\n            else:\n                adjusted_ckpt.update({f\"{model_prefix}{key}\": value for key, value in ckpt.items()})\n\n        if not adjusted_ckpt:\n            adjusted_ckpt = ckpt\n    else:\n        adjusted_ckpt = ckpt\n\n    # Remove ignored layers if specified\n    if layers_to_ignore:\n        for layer in layers_to_ignore:\n            adjusted_ckpt.pop(layer)\n\n    # Verify overlap between model and checkpoint keys\n    model_keys = list(model.state_dict().keys())\n    ckpt_keys = list(adjusted_ckpt.keys())\n    if not set(model_keys) &amp; set(ckpt_keys):\n        raise ValueError(\n            \"There is no overlap between checkpoint's and model's state_dict.\"\n            f\"\\nModel keys: {model_keys[0] + ', ..., ' + model_keys[-1] if model_keys else '[]'}\"\n            f\"\\nCheckpoint keys: {ckpt_keys[0] + ', ..., ' + ckpt_keys[-1] if ckpt_keys else '[]'}\"\n        )\n    # Load state dict and handle incompatible keys\n    incompatible_keys = model.load_state_dict(adjusted_ckpt, strict=False)\n    if incompatible_keys.missing_keys or incompatible_keys.unexpected_keys:\n        logger.info(f\"Encountered incompatible keys during checkpoint loading. If intended, ignore.\\n{incompatible_keys}\")\n    else:\n        logger.info(\"Checkpoint loaded successfully.\")\n\n    return model\n</code></pre>"},{"location":"reference/utils/model/#lighter.utils.model.remove_n_last_layers_sequentially","title":"<code>remove_n_last_layers_sequentially(model, num_layers=1)</code>","text":"<p>Removes a specified number of layers from the end of a model and returns it as a Sequential model.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Module()</code> <p>The model to modify.</p> required <code>num_layers</code> <p>The number of layers to remove from the end.</p> <code>1</code> <p>Returns:</p> Name Type Description <code>Sequential</code> <code>Sequential</code> <p>The modified model as a Sequential container.</p> Source code in <code>lighter/utils/model.py</code> <pre><code>def remove_n_last_layers_sequentially(model: Module(), num_layers=1) -&gt; Sequential:\n    \"\"\"\n    Removes a specified number of layers from the end of a model and returns it as a Sequential model.\n\n    Args:\n        model: The model to modify.\n        num_layers: The number of layers to remove from the end.\n\n    Returns:\n        Sequential: The modified model as a Sequential container.\n    \"\"\"\n    return Sequential(*list(model.children())[:-num_layers])\n</code></pre>"},{"location":"reference/utils/model/#lighter.utils.model.replace_layer_with","title":"<code>replace_layer_with(model, layer_name, new_layer)</code>","text":"<p>Replaces a specified layer in a PyTorch model with a new layer.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Module</code> <p>The model to modify.</p> required <code>layer_name</code> <code>str</code> <p>The name of the layer to replace, using dot notation if necessary (e.g. \"layer10.fc.weights\").</p> required <code>new_layer</code> <code>Module</code> <p>The new layer to insert.</p> required <p>Returns:</p> Name Type Description <code>Module</code> <code>Module</code> <p>The modified model with the new layer.</p> Source code in <code>lighter/utils/model.py</code> <pre><code>def replace_layer_with(model: Module, layer_name: str, new_layer: Module) -&gt; Module:\n    \"\"\"\n    Replaces a specified layer in a PyTorch model with a new layer.\n\n    Args:\n        model: The model to modify.\n        layer_name: The name of the layer to replace,\n            using dot notation if necessary (e.g. \"layer10.fc.weights\").\n        new_layer: The new layer to insert.\n\n    Returns:\n        Module: The modified model with the new layer.\n    \"\"\"\n    setattr_dot_notation(model, layer_name, new_layer)\n    return model\n</code></pre>"},{"location":"reference/utils/model/#lighter.utils.model.replace_layer_with_identity","title":"<code>replace_layer_with_identity(model, layer_name)</code>","text":"<p>Replaces a specified layer in a PyTorch model with an Identity layer.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Module</code> <p>The model to modify.</p> required <code>layer_name</code> <code>str</code> <p>The name of the layer to replace with an Identity layer, using dot notation if necessary (e.g. \"layer10.fc.weights\").</p> required <p>Returns:</p> Name Type Description <code>Module</code> <code>Module</code> <p>The modified model with the Identity layer.</p> Source code in <code>lighter/utils/model.py</code> <pre><code>def replace_layer_with_identity(model: Module, layer_name: str) -&gt; Module:\n    \"\"\"\n    Replaces a specified layer in a PyTorch model with an Identity layer.\n\n    Args:\n        model: The model to modify.\n        layer_name: The name of the layer to replace with an Identity layer,\n            using dot notation if necessary (e.g. \"layer10.fc.weights\").\n\n    Returns:\n        Module: The modified model with the Identity layer.\n    \"\"\"\n    return replace_layer_with(model, layer_name, Identity())\n</code></pre>"},{"location":"reference/utils/types/","title":"types","text":"<ul> <li>enums</li> <li>containers</li> </ul>"},{"location":"reference/utils/types/containers/","title":"containers","text":""},{"location":"reference/utils/types/containers/#lighter.utils.types.containers.Adapters","title":"<code>Adapters</code>  <code>dataclass</code>","text":"<p>Root configuration class for all adapters across different modes.</p> Source code in <code>lighter/utils/types/containers.py</code> <pre><code>@nested\n@dataclass\nclass Adapters:\n    \"\"\"Root configuration class for all adapters across different modes.\"\"\"\n\n    train: Train = field(default_factory=Train)\n    val: Val = field(default_factory=Val)\n    test: Test = field(default_factory=Test)\n    predict: Predict = field(default_factory=Predict)\n</code></pre>"},{"location":"reference/utils/types/containers/#lighter.utils.types.containers.Predict","title":"<code>Predict</code>  <code>dataclass</code>","text":"<p>Predict mode sub-dataclass for Adapters.</p> Source code in <code>lighter/utils/types/containers.py</code> <pre><code>@dataclass\nclass Predict:\n    \"\"\"Predict mode sub-dataclass for Adapters.\"\"\"\n\n    batch: BatchAdapter = field(default_factory=lambda: BatchAdapter(input_accessor=lambda batch: batch))\n    logging: LoggingAdapter = field(default_factory=LoggingAdapter)\n</code></pre>"},{"location":"reference/utils/types/containers/#lighter.utils.types.containers.Test","title":"<code>Test</code>  <code>dataclass</code>","text":"<p>Test mode sub-dataclass for Adapters.</p> Source code in <code>lighter/utils/types/containers.py</code> <pre><code>@dataclass\nclass Test:\n    \"\"\"Test mode sub-dataclass for Adapters.\"\"\"\n\n    batch: BatchAdapter = field(default_factory=lambda: BatchAdapter(input_accessor=0, target_accessor=1))\n    metrics: MetricsAdapter = field(default_factory=lambda: MetricsAdapter(pred_argument=0, target_argument=1))\n    logging: LoggingAdapter = field(default_factory=LoggingAdapter)\n</code></pre>"},{"location":"reference/utils/types/containers/#lighter.utils.types.containers.Train","title":"<code>Train</code>  <code>dataclass</code>","text":"<p>Train mode sub-dataclass for Adapters.</p> Source code in <code>lighter/utils/types/containers.py</code> <pre><code>@dataclass\nclass Train:\n    \"\"\"Train mode sub-dataclass for Adapters.\"\"\"\n\n    batch: BatchAdapter = field(default_factory=lambda: BatchAdapter(input_accessor=0, target_accessor=1))\n    criterion: CriterionAdapter = field(default_factory=lambda: CriterionAdapter(pred_argument=0, target_argument=1))\n    metrics: MetricsAdapter = field(default_factory=lambda: MetricsAdapter(pred_argument=0, target_argument=1))\n    logging: LoggingAdapter = field(default_factory=LoggingAdapter)\n</code></pre>"},{"location":"reference/utils/types/containers/#lighter.utils.types.containers.Val","title":"<code>Val</code>  <code>dataclass</code>","text":"<p>Val mode sub-dataclass for Adapters.</p> Source code in <code>lighter/utils/types/containers.py</code> <pre><code>@dataclass\nclass Val:\n    \"\"\"Val mode sub-dataclass for Adapters.\"\"\"\n\n    batch: BatchAdapter = field(default_factory=lambda: BatchAdapter(input_accessor=0, target_accessor=1))\n    criterion: CriterionAdapter = field(default_factory=lambda: CriterionAdapter(pred_argument=0, target_argument=1))\n    metrics: MetricsAdapter = field(default_factory=lambda: MetricsAdapter(pred_argument=0, target_argument=1))\n    logging: LoggingAdapter = field(default_factory=LoggingAdapter)\n</code></pre>"},{"location":"reference/utils/types/containers/#lighter.utils.types.containers.nested","title":"<code>nested(cls)</code>","text":"<p>Decorator to handle nested dataclass creation. Example:     <pre><code>@nested\n@dataclass\nclass Example:\n    ...\n</code></pre></p> Source code in <code>lighter/utils/types/containers.py</code> <pre><code>def nested(cls):\n    \"\"\"\n    Decorator to handle nested dataclass creation.\n    Example:\n        ```\n        @nested\n        @dataclass\n        class Example:\n            ...\n        ```\n    \"\"\"\n    original_init = cls.__init__\n\n    def __init__(self, *args, **kwargs):\n        for f in fields(cls):\n            if is_dataclass(f.type) and f.name in kwargs:\n                kwargs[f.name] = f.type(**kwargs[f.name])\n        original_init(self, *args, **kwargs)\n\n    cls.__init__ = __init__\n    return cls\n</code></pre>"},{"location":"reference/utils/types/enums/","title":"enums","text":""},{"location":"reference/utils/types/enums/#lighter.utils.types.enums.StrEnum","title":"<code>StrEnum</code>","text":"<p>               Bases: <code>str</code>, <code>Enum</code></p> <p>Enum class that inherits from str. This allows for the enum values to be accessed as strings.</p> Source code in <code>lighter/utils/types/enums.py</code> <pre><code>class StrEnum(str, Enum):\n    \"\"\"\n    Enum class that inherits from str. This allows for the enum values to be accessed as strings.\n    \"\"\"\n\n    # Remove this class when Python 3.10 support is dropped, as Python &gt;=3.11 has StrEnum built-in.\n    def __str__(self) -&gt; str:\n        return str(self.value)\n</code></pre>"}]}