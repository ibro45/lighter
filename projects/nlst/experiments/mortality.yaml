project: /home/ibro/Projects/lightningbringer/projects/nlst

trainer:
    _target_: pytorch_lightning.Trainer
    benchmark: True
    max_epochs: 100
    check_val_every_n_epoch: 5
    gpus: 2
    strategy: ddp
    precision: 16
    accelerator: gpu
    log_every_n_steps: 50
    logger:
        _target_: lightningbringer.logger.LightningBringerLogger
        save_dir: ${project}/logs/${now:}
        project: mortality
    callbacks:
        - _target_: pytorch_lightning.callbacks.ModelCheckpoint
          dirpath: ${trainer.logger.save_dir}/checkpoints
          monitor: val/loss
          mode: min
          verbose: True
          save_last: True

system:
    _target_: lightningbringer.System
    batch_size: 4
    pin_memory: True
    num_workers: 8
    log_input_as: null
    # log_pred_as: scalar  # TODO: breaking because it can't log a batch of scalars
    # log_target_as: scalar

    model:
        _target_: project.models.EfficientNet
        model_name: efficientnet-b5
        pretrained: False
        spatial_dims: 3
        in_channels: 1
        num_classes: 1
        last_activation:
            _target_: torch.nn.Identity  # Because using BCEWithLogitsLoss

    cast_target_dtype_to: torch.float
    criterion:
        _target_: torch.nn.BCEWithLogitsLoss
        # Probably not necessary with ImbalancedSampler
        # pos_weight:
        #     # pos_weight needs to be a tensor, instantiating one here
        #     _target_: torch.tensor
        #     _args_:
        #         - 1 # 8.05626598465473  # Calculated over whole dataset

    optimizers:
        _target_: torch.optim.Adam
        lr: 0.005
        weight_decay: 1e-5

    train_metrics:
        - _target_: torchmetrics.Accuracy
        - _target_: torchmetrics.F1Score

    val_metrics:
        - _target_: torchmetrics.Accuracy
          num_classes: 1
          average: weighted
        - _target_: torchmetrics.F1Score
          num_classes: 1
          average: weighted

    test_metrics:
        - _target_: torchmetrics.Accuracy
          num_classes: 1
          average: weighted
        - _target_: torchmetrics.F1Score
          num_classes: 1
          average: weighted

    train_dataset:
        _target_: project.datasets.NLSTDataset
        mode: train
        root_dir: /media/ibro/data_1/NLST/
        label: fup_days
        hounsfield_units_range: [-1000, 400]
        patch_size: [128, 304, 392]  # 96 [128, 304, 392]

    train_sampler:
        _target_: torchsampler.ImbalancedDatasetSampler
        # TODO: Workaround, it will reinstantiate it but that's ok in this case (wait for Hydra support)
        dataset: ${system.train_dataset}

    val_dataset:
        _target_: project.datasets.NLSTDataset
        mode: tune
        root_dir: /media/ibro/data_1/NLST/
        label: fup_days
        hounsfield_units_range: ${system.train_dataset.hounsfield_units_range}
        patch_size: ${system.train_dataset.patch_size}

    test_dataset:
        _target_: project.datasets.NLSTDataset
        mode: test
        root_dir: /media/ibro/data_1/NLST/
        label: fup_days
        hounsfield_units_range: ${system.train_dataset.hounsfield_units_range}
        patch_size: ${system.train_dataset.patch_size}

    patch_based_inferer:
        _target_: patch_inferers.AvgPatchInferer
        batch_size: ${system.batch_size}
        patch_size: ${system.train_dataset.patch_size}
        patch_overlap: [32, 76, 98]  # 0.25 * [128, 304, 392]
