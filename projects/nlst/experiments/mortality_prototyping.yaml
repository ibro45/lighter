project: /home/ibro/Projects/lightningbringer/projects/nlst

trainer:
    _target_: pytorch_lightning.Trainer
    max_epochs: 100
    check_val_every_n_epoch: 1
    accelerator: gpu
    devices: 2
    strategy: ddp
    precision: 16
    # deterministic: True
    log_every_n_steps: 5
    # default_root_dir: ${project}/logs
    logger:
        - _target_: pytorch_lightning.loggers.WandbLogger
          save_dir: ${project}/prototyping/logs/${now:}
          project: mortality
        - _target_: pytorch_lightning.loggers.TensorBoardLogger
          save_dir: ${trainer.logger.0.save_dir}
          name: mortality
    callbacks:
        - _target_: pytorch_lightning.callbacks.ModelCheckpoint
          dirpath: ${trainer.logger.0.save_dir}/checkpoints
          monitor: val/loss
          mode: min
          verbose: True
          save_last: True

system:
    _target_: lightningbringer.System
    debug: True
    batch_size: 2
    pin_memory: True
    num_workers: 2
    log_input_as: null
    # log_pred_as: scalar  # TODO: breaking because it can't log a batch of scalars
    # log_target_as: scalar

    model:
        _target_: project.models.EfficientNet
        model_name: efficientnet-b5
        pretrained: False
        spatial_dims: 3
        in_channels: 1
        num_classes: 1
        last_activation:
            _target_: torch.nn.Identity  # Because using BCEWithLogitsLoss

    cast_target_dtype_to: torch.float
    criterion:
        _target_: torch.nn.BCEWithLogitsLoss
        # Probably not necessary with ImbalancedSampler
        pos_weight:
            # pos_weight needs to be a tensor, instantiating one here
            _target_: torch.tensor
            _args_:
                - 8.05626598465473  # Calculated over whole dataset

    optimizers:
        _target_: torch.optim.Adam
        lr: 0.001
        weight_decay: 1e-5

    train_metrics:
        - _target_: torchmetrics.Accuracy
        #   num_classes: 1
        #   average: weighted
        - _target_: torchmetrics.F1Score
        #   num_classes: 1
        #   average: weighted

    val_metrics:
        - _target_: torchmetrics.Accuracy
        #   num_classes: 1
        #   average: weighted
        - _target_: torchmetrics.F1Score
        #   num_classes: 1
        #   average: weighted

    train_dataset:
        _target_: project.datasets.NLSTDataset
        mode: train
        root_dir: /media/ibro/data_1/NLST/
        label: fup_days
        hounsfield_units_range: [-1000, 400]
        patch_size: [64, 256, 256]  # 96 [128, 304, 392]
        prototyping_num_scans: 30 #0

    # train_sampler:
    #     _target_: torchsampler.ImbalancedDatasetSampler
    #     # TODO: Workaround, it will reinstantiate it but that's ok in this case (wait for Hydra support),
    #     # https://github.com/facebookresearch/hydra/issues/1758
    #     dataset: ${system.train_dataset}

    val_dataset:
        _target_: project.datasets.NLSTDataset
        mode: tune
        root_dir: /media/ibro/data_1/NLST/
        label: fup_days
        hounsfield_units_range: ${system.train_dataset.hounsfield_units_range}
        patch_size: ${system.train_dataset.patch_size}
        prototyping_num_scans: 10

    test_dataset:
        _target_: project.datasets.NLSTDataset
        mode: test
        root_dir: /media/ibro/data_1/NLST/
        label: fup_days
        hounsfield_units_range: ${system.train_dataset.hounsfield_units_range}
        patch_size: ${system.train_dataset.patch_size}

    patch_based_inferer:
        _target_: patch_inferers.AvgPatchInferer
        batch_size: ${system.batch_size}
        patch_size: ${system.train_dataset.patch_size}
        patch_overlap: [4, 4, 4]
