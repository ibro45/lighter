project: /home/ibro/Projects/lighter/projects/nlst

trainer:
    _target_: pytorch_lightning.Trainer
    benchmark: True
    max_epochs: 100
    check_val_every_n_epoch: 1
    accelerator: gpu
    devices: 1
    # strategy:
    #     _target_: pytorch_lightning.strategies.ddp.DDPStrategy
    #     find_unused_parameters: False
    precision: 16
    log_every_n_steps: 50
    # logger:
    #     _target_: lighter.logger.LighterLogger
    #     save_dir: ${project}/logs/${now:}
    #     tensorboard: True
    #     wandb: True
    #     wandb_project: gender_nlst

    # callbacks:
    #     - _target_: pytorch_lightning.callbacks.ModelCheckpoint
    #       dirpath: ${trainer.logger.save_dir}/checkpoints
    #       monitor: val/loss
    #       mode: min
    #       verbose: True
    #       save_last: True
    #       save_on_train_epoch_end: True

system:
    _target_: lighter.LighterSystem
    batch_size: 8
    pin_memory: True
    num_workers: 3
    log_input_as: null
    # log_pred_as: scalar  # TODO: breaking because it can't log a batch of scalars
    # log_target_as: scalar

    model:
        _target_: project.models.EfficientNet
        model_name: efficientnet-b0
        pretrained: False
        dropout_rate: 0.2
        spatial_dims: 3
        in_channels: 1
        num_classes: 1
        last_activation:
            _target_: torch.nn.Identity  # Because using BCEWithLogitsLoss

    criterion:
        _target_: torch.nn.BCEWithLogitsLoss
        # # Disable ImbalancedSampler if using pos_weight
        # pos_weight:
        #     # pos_weight needs to be a tensor, instantiating one here
        #     _target_: torch.tensor
        #     _args_:
        #         - 7.739782016348774  # Calculated over the train set
    
    post_criterion_activation: ${import:torch.sigmoid}
    cast_target_dtype_to: ${import:torch.float}

    optimizers:
        _target_: torch.optim.Adam
        lr: 0.005
        weight_decay: 1e-5

    schedulers:
        # _target_: torch.optim.lr_scheduler.ExponentialLR
        # gamma: 0.95
    
        scheduler:
            _target_: torch.optim.lr_scheduler.ReduceLROnPlateau
        interval: epoch
        monitor: val/loss
        strict: True

    train_metrics:
        - _target_: torchmetrics.Precision
        - _target_: torchmetrics.Recall
        - _target_: torchmetrics.Specificity
        - _target_: torchmetrics.F1Score

    val_metrics: ${system.train_metrics}
        # - _target_: torchmetrics.Precision
        #   num_classes: 1
        #   average: weighted
        #   threshold: 0.2
        # - _target_: torchmetrics.Recall
        #   num_classes: 1
        #   average: weighted
        #   threshold: 0.2
        # - _target_: torchmetrics.Specificity
        #   num_classes: 1
        #   average: weighted
        #   threshold: 0.2
        # - _target_: torchmetrics.F1Score
        #   num_classes: 1
        #   average: weighted
        #   threshold: 0.2

    test_metrics: ${system.train_metrics}

    train_dataset:
        _target_: project.datasets.NLSTDataset
        mode: train
        root_dir: /media/ibro/data_1/NLST/
        label: gender
        hounsfield_units_range: [-1000, 400]
        patch_size: [128, 304, 392]  # 96 [128, 304, 392]
        # transform:
        #     _target_: monai.transforms.Compose
        #     transforms:
        #         - _target_: monai.transforms.RandGaussianSmooth
        #           prob: 0.2
        #         - _target_: monai.transforms.RandAffine
        #           prob: 0.3
        #         - _target_: monai.transforms.RandRotate
        #           prob: 0.3
        #           range_x: 0.3
        #           range_z: 0.3
        #           range_y: 0.3

    train_sampler:
        _target_: torchsampler.ImbalancedDatasetSampler
        # TODO: Workaround, it will reinstantiate it but that's ok in this case (wait for Hydra support),
        # https://github.com/facebookresearch/hydra/issues/1758
        dataset: ${system.train_dataset}

    val_dataset:
        _target_: project.datasets.NLSTDataset
        mode: tune
        root_dir: /media/ibro/data_1/NLST/
        label: ${system.train_dataset.label}
        hounsfield_units_range: ${system.train_dataset.hounsfield_units_range}
        patch_size: ${system.train_dataset.patch_size}

    test_dataset:
        _target_: project.datasets.NLSTDataset
        mode: test
        root_dir: /media/ibro/data_1/NLST/
        label: ${system.train_dataset.label}
        hounsfield_units_range: ${system.train_dataset.hounsfield_units_range}
        patch_size: ${system.train_dataset.patch_size}

    patch_based_inferer:
        _target_: patch_inferers.AvgPatchInferer
        batch_size: ${system.batch_size}
        patch_size: ${system.train_dataset.patch_size}
        patch_overlap: [32, 76, 98]  # 0.25 * [128, 304, 392]
